# -*- coding: utf-8 -*-
import os
import sys
import re
import json as json_module
import requests
import unicodedata
import repetable_processor as rp
from dotenv import load_dotenv
from datetime import datetime
from queries import get_demarche, get_dossier, get_demarche_dossiers, dossier_to_flat_data, format_complex_json_for_grist
from queries_graphql import get_demarche_dossiers_filtered


# Configuration du niveau de log
LOG_LEVEL = 1  # 0=minimal, 1=normal, 2=verbose

def log(message, level=1):
    """Fonction de log conditionnelle selon le niveau d√©fini"""
    if level <= LOG_LEVEL:
        print(message)

def log_verbose(message):
    """Log uniquement en mode verbose"""
    log(message, 2)

def log_error(message):
    """Log d'erreur (toujours affich√©)"""
    print(f"ERREUR: {message}")

# APR√àS avoir d√©fini les fonctions de log, importez le module schema_utils
try:
    from schema_utils import (
    get_demarche_schema, 
    create_columns_from_schema, 
    update_grist_tables_from_schema,
    get_demarche_schema_enhanced  # NOUVELLE FONCTION OPTIMIS√âE
)
    log("Module schema_utils trouv√© et charg√© avec succ√®s.")
except ImportError:
    log_error("Module schema_utils non trouv√©. La cr√©ation de sch√©ma avanc√©e ne sera pas disponible.")
    # Configuration de migration progressive
# Configuration : version optimis√©e stabilis√©e
def get_optimized_schema(demarche_number):
    """
    R√©cup√©ration optimis√©e du sch√©ma avec fallback automatique.
    """
    try:
        log("üìä R√©cup√©ration optimis√©e du sch√©ma")
        return get_demarche_schema_enhanced(demarche_number, prefer_robust=True)
    except Exception as e:
        log_error(f"Erreur version optimis√©e: {e}")
        log("üîÑ Fallback vers version classique")
        return get_demarche_schema(demarche_number)

def log_schema_improvements(schema, demarche_number):
    """Affiche les am√©liorations apport√©es par la nouvelle version"""
    if schema.get("metadata", {}).get("optimized"):
        log("üéØ AM√âLIORATIONS D√âTECT√âES:")
        revision_id = schema.get("metadata", {}).get("revision_id", "N/A")
        retrieved_at = schema.get("metadata", {}).get("retrieved_at", "N/A")
        log(f"   üîç R√©vision active: {revision_id}")
        log(f"   ‚è±Ô∏è  R√©cup√©r√© √†: {retrieved_at}")
        log("   üßπ Filtrage automatique des champs probl√©matiques activ√©")
        log("   üîÑ Gestion robuste des erreurs activ√©e")
        log("   üìä M√©tadonn√©es enrichies disponibles")

# Fonction pour supprimer les accents d'une cha√Æne de caract√®res
def normalize_column_name(name, max_length=50):
    """
    Normalise un nom de colonne pour Grist en garantissant des identifiants valides.
    Supprime les espaces en d√©but, fin et les espaces cons√©cutifs.
    
    Args:
        name: Le nom original de la colonne
        max_length: Longueur maximale autoris√©e (d√©faut: 50)
        
    Returns:
        str: Nom de colonne normalis√© pour Grist
    """
    if not name:
        return "column"
    
    # Supprimer les espaces en d√©but et fin, et remplacer les espaces cons√©cutifs par un seul espace
    import re
    name = name.strip()
    name = re.sub(r'\s+', ' ', name)
    
    # Supprimer les accents
    import unicodedata
    name = unicodedata.normalize('NFKD', name)
    name = ''.join([c for c in name if not unicodedata.combining(c)])
    
    # Convertir en minuscules et remplacer les caract√®res non alphanum√©riques par des underscores
    name = name.lower()
    name = re.sub(r'[^a-z0-9_]', '_', name)
    
    # √âliminer les underscores multiples cons√©cutifs
    name = re.sub(r'_+', '_', name)
    
    # √âliminer les underscores en d√©but et fin
    name = name.strip('_')
    
    # S'assurer que le nom commence par une lettre
    if not name or not name[0].isalpha():
        name = "col_" + (name or "")
    
    # Tronquer si n√©cessaire √† max_length caract√®res
    if len(name) > max_length:
        # G√©n√©rer un hash pour garantir l'unicit√©
        import hashlib
        hash_part = hashlib.md5(name.encode()).hexdigest()[:6]
        name = f"{name[:max_length-7]}_{hash_part}"
    
    return name

# 1. D'abord, ajoutez la fonction filter_record_to_existing_columns apr√®s les autres fonctions utilitaires

def filter_record_to_existing_columns(client, table_id, record):
    """
    Filtre un enregistrement pour ne garder que les colonnes existantes dans la table.
    
    Args:
        client: Instance de GristClient
        table_id: ID de la table Grist
        record: Dictionnaire de l'enregistrement √† filtrer
        
    Returns:
        dict: Enregistrement filtr√©
    """
    # R√©cup√©rer les colonnes existantes
    try:
        url = f"{client.base_url}/docs/{client.doc_id}/tables/{table_id}/columns"
        response = requests.get(url, headers=client.headers)
        
        if response.status_code != 200:
            log_error(f"Erreur lors de la r√©cup√©ration des colonnes: {response.status_code}")
            return record  # Retourner l'enregistrement tel quel en cas d'erreur
        
        columns_data = response.json()
        existing_columns = set()
        
        if "columns" in columns_data:
            for col in columns_data["columns"]:
                existing_columns.add(col.get("id"))
        
        log_verbose(f"Colonnes existantes dans la table {table_id}: {len(existing_columns)}")
        
        # Filtrer l'enregistrement
        filtered_record = {}
        for key, value in record.items():
            if key in existing_columns:
                filtered_record[key] = value
            else:
                log_verbose(f"  Colonne {key} ignor√©e car inexistante dans la table")
        
        # Toujours garder dossier_number pour les r√©f√©rences
        if "dossier_number" in record and "dossier_number" not in filtered_record:
            filtered_record["dossier_number"] = record["dossier_number"]
        
        return filtered_record
    
    except Exception as e:
        log_error(f"Erreur lors du filtrage de l'enregistrement: {str(e)}")
        return record  # Retourner l'enregistrement tel quel en cas d'erreur



def detect_column_types_from_multiple_dossiers(dossiers_data, problematic_ids=None):
    """
    D√©tecte les types de colonnes pour les tables Grist √† partir des donn√©es de plusieurs dossiers.
    """
    # Colonnes fixes pour la table des dossiers
    dossier_columns = [
        {"id": "dossier_id", "type": "Text"},
        {"id": "number", "type": "Int"},
        {"id": "state", "type": "Text"},
        {"id": "date_depot", "type": "DateTime"},
        {"id": "date_derniere_modification", "type": "DateTime"},
        {"id": "date_traitement", "type": "DateTime"},
        {"id": "demandeur_type", "type": "Text"},
        {"id": "demandeur_civilite", "type": "Text"},
        {"id": "demandeur_nom", "type": "Text"},
        {"id": "demandeur_prenom", "type": "Text"},
        {"id": "demandeur_email", "type": "Text"},
        {"id": "demandeur_siret", "type": "Text"},
        {"id": "entreprise_raison_sociale", "type": "Text"},
        {"id": "usager_email", "type": "Text"},
        {"id": "groupe_instructeur_id", "type": "Text"},
        {"id": "groupe_instructeur_number", "type": "Int"},
        {"id": "groupe_instructeur_label", "type": "Text"},
        {"id": "supprime_par_usager", "type": "Bool"},
        {"id": "date_suppression", "type": "DateTime"},
        {"id": "label_names", "type": "Text"},
        {"id": "labels_json", "type": "Text"}
    ]

    # Colonnes de base pour la table des champs
    champ_columns = [
        {"id": "dossier_number", "type": "Int"},
    ]
    
    # Colonnes de base pour la table des annotations
    annotation_columns = [
        {"id": "dossier_number", "type": "Int"},
    ]

    # Dictionnaires pour suivre les types uniques
    unique_champ_columns = {}
    unique_annotation_columns = {}

    # Indicateurs de pr√©sence
    has_repetable_blocks = False
    has_carto_fields = False

    # Fonction pour d√©terminer le type de colonne
    def determine_column_type(value):
        if value is None:
            return "Text"
        elif isinstance(value, int):
            return "Int"
        elif isinstance(value, float):
            return "Numeric"
        elif isinstance(value, bool):
            return "Bool"
        elif isinstance(value, (datetime, str)) and (
            isinstance(value, datetime) or 
            any(fmt in value for fmt in ["-", "T", ":"])
        ):
            return "DateTime"
        else:
            return "Text"

    # Fonction r√©cursive pour v√©rifier les champs
    def check_for_repetable_and_carto(champs):
        nonlocal has_repetable_blocks, has_carto_fields
        
        for champ in champs:
            # Ignorer les types HeaderSectionChamp et ExplicationChamp
            if champ["__typename"] in ["HeaderSectionChamp", "ExplicationChamp"]:
                continue
                
            if champ["__typename"] == "RepetitionChamp":
                has_repetable_blocks = True
                # V√©rifier les champs √† l'int√©rieur des blocs r√©p√©tables
                for row in champ.get("rows", []):
                    if "champs" in row:
                        for field in row["champs"]:
                            # Ignorer les types HeaderSectionChamp et ExplicationChamp dans les blocs r√©p√©tables
                            if field["__typename"] in ["HeaderSectionChamp", "ExplicationChamp"]:
                                continue
                                
                            if field["__typename"] == "CarteChamp":
                                has_carto_fields = True
                                return  # Sortir d√®s qu'on a trouv√© les deux types
            elif champ["__typename"] == "CarteChamp":
                has_carto_fields = True

    # Analyser tous les dossiers pour d√©tecter les colonnes et les types de champs
    for dossier_data in dossiers_data:
        # Utiliser dossier_to_flat_data avec exclude_repetition_champs=True
        # pour exclure les blocs r√©p√©tables de la table des champs
        flat_data = dossier_to_flat_data(dossier_data, exclude_repetition_champs=True, problematic_ids=problematic_ids)
        
        # Collecter les champs
        for champ in flat_data["champs"]:
            # Ignorer les champs de type HeaderSectionChamp et ExplicationChamp
            if champ["type"] in ["HeaderSectionChamp", "ExplicationChamp"]:
                continue

            # Ignorer les champs dont l'ID est dans la liste des probl√©matiques
            if problematic_ids and champ.get("id") in problematic_ids:
                continue
                
            champ_label = normalize_column_name(champ["label"])
            
            if champ_label not in unique_champ_columns:
                column_type = determine_column_type(champ.get("value"))
                unique_champ_columns[champ_label] = column_type

    

        # Collecter les annotations
        for annotation in flat_data["annotations"]:
            # Ignorer les annotations de type HeaderSectionChamp et ExplicationChamp
            if annotation["type"] in ["HeaderSectionChamp", "ExplicationChamp"]:
                continue
                
            # Enlever le pr√©fixe "annotation_" pour le nom de colonne dans la table des annotations
            original_label = annotation["label"]
            if original_label.startswith("annotation_"):
                annotation_label = normalize_column_name(original_label[11:])  # enlever "annotation_"
            else:
                annotation_label = normalize_column_name(original_label)
            
            if annotation_label not in unique_annotation_columns:
                column_type = determine_column_type(annotation.get("value"))
                unique_annotation_columns[annotation_label] = column_type
        
        # V√©rifier la pr√©sence de blocs r√©p√©tables et de champs cartographiques
        check_for_repetable_and_carto(dossier_data.get("champs", []))
        if not (has_repetable_blocks and has_carto_fields):  # Continuer seulement si on n'a pas encore trouv√© les deux
            check_for_repetable_and_carto(dossier_data.get("annotations", []))
        
        if has_repetable_blocks and has_carto_fields:
            break  # Sortir de la boucle si on a d√©j√† trouv√© les deux types

    # Ajouter les colonnes uniques d√©tect√©es
    for col_name, col_type in unique_champ_columns.items():
        champ_columns.append({
            "id": col_name,
            "type": col_type
        })
        
    # Ajouter les colonnes uniques d'annotations d√©tect√©es
    for col_name, col_type in unique_annotation_columns.items():
        annotation_columns.append({
            "id": col_name,
            "type": col_type
        })

    # Pr√©parer le r√©sultat
    result = {
        "dossier": dossier_columns,
        "champs": champ_columns,
        "annotations": annotation_columns,
        "has_repetable_blocks": has_repetable_blocks,
        "has_carto_fields": has_carto_fields
    }
    
    # Ne d√©tecter les colonnes des blocs r√©p√©tables que si n√©cessaire
    if has_repetable_blocks:
        try:
            import repetable_processor as rp
            repetable_columns = rp.detect_repetable_columns_from_multiple_dossiers(dossiers_data)
            result["repetable_rows"] = repetable_columns
        except Exception as e:
            log_error(f"Erreur lors de la d√©tection des colonnes des blocs r√©p√©tables: {str(e)}")
            import traceback
            traceback.print_exc()
            # Fournir au moins une structure de base en cas d'erreur
            result["repetable_rows"] = [
                {"id": "dossier_number", "type": "Int"},
                {"id": "block_label", "type": "Text"},
                {"id": "block_row_index", "type": "Int"},
                {"id": "block_row_id", "type": "Text"}
            ]

    return result

def get_problematic_descriptor_ids(demarche_number):
    """
    R√©cup√®re les IDs des descripteurs de champs probl√©matiques (HeaderSectionChamp et ExplicationChamp)
    pour une d√©marche donn√©e.
    """
    from queries_config import API_TOKEN, API_URL
    import requests
    
    # Requ√™te GraphQL pour r√©cup√©rer les descripteurs de champs
    query = """
    query getDemarche($demarcheNumber: Int!) {
      demarche(number: $demarcheNumber) {
        activeRevision {
          champDescriptors {
            __typename
            id
            type
          }
        }
      }
    }
    """
    
    headers = {
        "Authorization": f"Bearer {API_TOKEN}",
        "Content-Type": "application/json"
    }
    
    response = requests.post(
        API_URL,
        json={"query": query, "variables": {"demarcheNumber": int(demarche_number)}},
        headers=headers
    )
    
    response.raise_for_status()
    result = response.json()
    
    problematic_ids = set()
    
    # V√©rifier les erreurs
    if "errors" in result:
        log_error(f"GraphQL errors: {', '.join([error.get('message', 'Unknown error') for error in result['errors']])}")
        return problematic_ids
    
    # Extraire les IDs des champs probl√©matiques
    if (result.get("data") and result["data"].get("demarche") and 
        result["data"]["demarche"].get("activeRevision") and 
        result["data"]["demarche"]["activeRevision"].get("champDescriptors")):
        
        descriptors = result["data"]["demarche"]["activeRevision"]["champDescriptors"]
        
        for descriptor in descriptors:
            if descriptor.get("type") in ["header_section", "explication"] or descriptor.get("__typename") in ["HeaderSectionChampDescriptor", "ExplicationChampDescriptor"]:
                problematic_ids.add(descriptor.get("id"))
    
    log(f"Nombre de descripteurs probl√©matiques identifi√©s: {len(problematic_ids)}")
    return problematic_ids

def format_value_for_grist(value, value_type):
    if value is None:
        return None

    if value_type == "DateTime":
        if isinstance(value, str):
            if value:
                for fmt in ["%Y-%m-%dT%H:%M:%S.%fZ", "%Y-%m-%dT%H:%M:%SZ", "%Y-%m-%d %H:%M:%S", "%Y-%m-%d"]:
                    try:
                        dt = datetime.strptime(value, fmt)
                        return dt.strftime("%Y-%m-%dT%H:%M:%SZ")
                    except ValueError:
                        continue
            return value
        return value

    if value_type == "Text":
        if isinstance(value, str) and len(value) > 1000:
            return value[:1000] + "..."
        return str(value)

    if value_type in ["Int", "Numeric"]:
        try:
            if value_type == "Int":
                return int(float(value)) if value else None
            return float(value) if value else None
        except (ValueError, TypeError):
            return None

    if value_type == "Bool":
        if isinstance(value, bool):
            return value
        if isinstance(value, str):
            return value.lower() in ["true", "1", "yes", "oui", "vrai"]
        return bool(value)

    return value

class ColumnCache:
    """
    Classe pour mettre en cache les informations sur les colonnes de tables Grist,
    √©vitant ainsi des requ√™tes r√©p√©t√©es pour obtenir la structure des tables.
    """
    def __init__(self, client):
        self.client = client
        self.columns_cache = {}  # {table_id: {column_id: column_type}}
    
    def get_columns(self, table_id, force_refresh=False):
        """
        R√©cup√®re les colonnes d'une table, en utilisant le cache si disponible.
        
        Args:
            table_id: ID de la table Grist
            force_refresh: Force la r√©cup√©ration depuis l'API m√™me si en cache
            
        Returns:
            set: Ensemble des IDs de colonnes
        """
        if table_id not in self.columns_cache or force_refresh:
            log_verbose(f"R√©cup√©ration des colonnes pour la table {table_id}")
            url = f"{self.client.base_url}/docs/{self.client.doc_id}/tables/{table_id}/columns"
            response = requests.get(url, headers=self.client.headers)
            
            if response.status_code == 200:
                columns_data = response.json()
                column_ids = set()
                column_types = {}
                
                if "columns" in columns_data:
                    for col in columns_data["columns"]:
                        col_id = col.get("id")
                        col_type = col.get("type", "Text")
                        if col_id:
                            column_ids.add(col_id)
                            column_types[col_id] = col_type
                
                self.columns_cache[table_id] = {"ids": column_ids, "types": column_types}
                log_verbose(f"  {len(column_ids)} colonnes en cache pour {table_id}")
            else:
                log_error(f"Erreur lors de la r√©cup√©ration des colonnes: {response.status_code}")
                self.columns_cache[table_id] = {"ids": set(), "types": {}}
        
        return self.columns_cache[table_id]["ids"]
    
    def get_column_type(self, table_id, column_id):
        """
        R√©cup√®re le type d'une colonne sp√©cifique.
        
        Args:
            table_id: ID de la table Grist
            column_id: ID de la colonne
            
        Returns:
            str: Type de la colonne ou "Text" par d√©faut
        """
        if table_id not in self.columns_cache:
            self.get_columns(table_id)
        
        return self.columns_cache[table_id]["types"].get(column_id, "Text")
    
    def add_missing_columns(self, table_id, missing_columns, column_types=None):
        """
        Ajoute les colonnes manquantes et met √† jour le cache.
        
        Args:
            table_id: ID de la table
            missing_columns: Liste des noms de colonnes manquantes
            column_types: Dictionnaire des types de colonnes
            
        Returns:
            tuple: (bool succ√®s, dict mapping des noms de colonnes)
        """
        if not missing_columns:
            return True, {}
        
        # Obtenir les colonnes existantes
        existing_columns = self.get_columns(table_id)
        
        # Ne garder que les colonnes r√©ellement manquantes
        columns_to_add = []
        column_mapping = {}
        
        for col_name in missing_columns:
            normalized_col_name = normalize_column_name(col_name)
            column_mapping[col_name] = normalized_col_name
            
            if normalized_col_name not in existing_columns:
                # D√©terminer le type
                col_type = "Text"
                if column_types and "champs" in column_types:
                    champ_column_types = {col["id"]: col["type"] for col in column_types["champs"]}
                    if col_name in champ_column_types:
                        col_type = champ_column_types[col_name]
                
                columns_to_add.append({"id": normalized_col_name, "type": col_type})
        
        if not columns_to_add:
            return True, column_mapping
        
        # Ajouter les colonnes
        url = f"{self.client.base_url}/docs/{self.client.doc_id}/tables/{table_id}/columns"
        payload = {"columns": columns_to_add}
        
        log(f"  Ajout de {len(columns_to_add)} colonnes √† la table {table_id}")
        response = requests.post(url, headers=self.client.headers, json=payload)
        
        if response.status_code == 200:
            log(f"  {len(columns_to_add)} colonnes ajout√©es avec succ√®s √† la table {table_id}")
            
            # Mettre √† jour le cache
            if table_id in self.columns_cache:
                for col in columns_to_add:
                    self.columns_cache[table_id]["ids"].add(col["id"])
                    self.columns_cache[table_id]["types"][col["id"]] = col["type"]
            
            return True, column_mapping
        else:
            log_error(f"  Erreur lors de l'ajout des colonnes: {response.status_code} - {response.text}")
            return False, column_mapping

import concurrent.futures
import time

def fetch_dossiers_in_parallel(dossier_numbers, max_workers=2, timeout=120):
    """
    R√©cup√®re plusieurs dossiers en parall√®le.
    
    Args:
        dossier_numbers: Liste des num√©ros de dossiers √† r√©cup√©rer
        max_workers: Nombre maximum de threads √† utiliser
        timeout: D√©lai d'attente maximum par dossier en secondes
        
    Returns:
        dict: Dictionnaire {dossier_number: dossier_data}
    """
    results = {}
    errors = []
    
    def fetch_dossier(dossier_number):
        try:
            start_time = time.time()
            dossier_data = get_dossier(dossier_number)
            elapsed = time.time() - start_time
            log_verbose(f"Dossier {dossier_number} r√©cup√©r√© en {elapsed:.2f}s")
            return dossier_number, dossier_data
        except Exception as e:
            log_error(f"Erreur lors de la r√©cup√©ration du dossier {dossier_number}: {str(e)}")
            return dossier_number, None
    
    log(f"R√©cup√©ration en parall√®le de {len(dossier_numbers)} dossiers avec {max_workers} workers...")
    
    # Utiliser ThreadPoolExecutor pour le parall√©lisme
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Soumettre les t√¢ches
        future_to_dossier = {
            executor.submit(fetch_dossier, dossier_num): dossier_num 
            for dossier_num in dossier_numbers
        }
        
        # Traiter les r√©sultats au fur et √† mesure
        for future in concurrent.futures.as_completed(future_to_dossier, timeout=timeout):
            dossier_num = future_to_dossier[future]
            try:
                dossier_num, dossier_data = future.result()
                if dossier_data:
                    results[dossier_num] = dossier_data
                else:
                    errors.append(dossier_num)
            except Exception as e:
                log_error(f"Exception pour le dossier {dossier_num}: {str(e)}")
                errors.append(dossier_num)
    
    success_rate = len(results) / len(dossier_numbers) * 100 if dossier_numbers else 0
    log(f"R√©cup√©ration parall√®le termin√©e: {len(results)}/{len(dossier_numbers)} dossiers r√©cup√©r√©s ({success_rate:.1f}%)")
    
    if errors:
        log(f"√âchecs: {len(errors)} dossiers n'ont pas pu √™tre r√©cup√©r√©s")
    
    return results

def process_dossiers_in_parallel(client, dossier_data_dict, table_ids, column_types, problematic_ids=None, max_workers=3):
    """
    Traite plusieurs dossiers en parall√®le pour Grist.
    
    Args:
        client: Instance de GristClient (doit √™tre thread-safe)
        dossier_data_dict: Dictionnaire {dossier_number: dossier_data}
        table_ids: IDs des tables Grist
        column_types: Types de colonnes
        problematic_ids: IDs des descripteurs √† filtrer
        max_workers: Nombre maximum de threads
        
    Returns:
        tuple: (success_count, error_count)
    """
    results = {}
    
    def process_single_dossier(dossier_number, dossier_data):
        try:
            success = process_dossier_for_grist(client, dossier_data, table_ids, column_types, problematic_ids)
            return dossier_number, success
        except Exception as e:
            log_error(f"Exception lors du traitement du dossier {dossier_number}: {str(e)}")
            import traceback
            traceback.print_exc()
            return dossier_number, False
    
    log(f"Traitement en parall√®le de {len(dossier_data_dict)} dossiers avec {max_workers} workers...")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Soumettre les t√¢ches
        future_to_dossier = {
            executor.submit(process_single_dossier, dossier_num, dossier_data): dossier_num
            for dossier_num, dossier_data in dossier_data_dict.items()
        }
        
        # Traiter les r√©sultats au fur et √† mesure
        for i, future in enumerate(concurrent.futures.as_completed(future_to_dossier)):
            dossier_num = future_to_dossier[future]
            try:
                dossier_num, success = future.result()
                results[dossier_num] = success
                
                # Afficher la progression
                if (i+1) % 10 == 0 or i+1 == len(dossier_data_dict):
                    success_so_far = sum(1 for result in results.values() if result)
                    log(f"Progression: {i+1}/{len(dossier_data_dict)} dossiers trait√©s, {success_so_far} succ√®s")
            except Exception as e:
                log_error(f"Exception pour le dossier {dossier_num}: {str(e)}")
                results[dossier_num] = False
    
    success_count = sum(1 for result in results.values() if result)
    error_count = len(results) - success_count
    
    return success_count, error_count


# Fonction pour r√©cup√©rer les labels d'un dossier sp√©cifique
def get_dossier_labels(dossier_number):
    """R√©cup√®re uniquement les labels d'un dossier sp√©cifique"""
    from queries_config import API_TOKEN, API_URL
    
    query = """
    query GetDossierLabels($dossierNumber: Int!) {
        dossier(number: $dossierNumber) {
            id
            number
            labels {
                id
                name
                color
            }
        }
    }
    """
    
    variables = {"dossierNumber": int(dossier_number)}
    
    headers = {
        "Authorization": f"Bearer {API_TOKEN}",
        "Content-Type": "application/json"
    }
    
    response = requests.post(
        API_URL,
        json={"query": query, "variables": variables},
        headers=headers
    )
    
    if response.status_code != 200:
        log_error(f"Erreur HTTP lors de la r√©cup√©ration des labels: {response.status_code}")
        return None
    
    result = response.json()
    
    if "errors" in result:
        log_error(f"Erreurs GraphQL lors de la r√©cup√©ration des labels")
        return None
    
    return result.get("data", {}).get("dossier", {}).get("labels", [])

# Fonction pour ajouter des colonnes manquantes √† une table Grist
def add_missing_columns_to_table(client, table_id, missing_columns, column_types=None):
    """
    Ajoute les colonnes manquantes √† une table Grist existante.
    V√©rifie que l'ajout a bien fonctionn√© avant de continuer.
    
    Args:
        client: Instance de GristClient
        table_id: ID de la table
        missing_columns: Liste des noms de colonnes manquantes
        column_types: Dictionnaire des types de colonnes (optionnel)
    
    Returns:
        tuple: (bool succ√®s, dict mapping des noms de colonnes)
    """
    try:
        if not missing_columns:
            return True, {}  # Rien √† ajouter
            
        # Mapping des noms originaux vers les noms normalis√©s
        column_mapping = {}
        columns_to_add = []
        
        for col_name in missing_columns:
            # Normaliser le nom de colonne
            normalized_col_name = normalize_column_name(col_name)
            column_mapping[col_name] = normalized_col_name
            
            # D√©terminer le type de colonne (Text par d√©faut)
            col_type = "Text"
            
            # Si column_types est fourni, essayer de trouver le type
            if column_types and "champs" in column_types:
                champ_column_types = {col["id"]: col["type"] for col in column_types["champs"]}
                if col_name in champ_column_types:
                    col_type = champ_column_types[col_name]
            
            # Ajouter la d√©finition de colonne
            columns_to_add.append({"id": normalized_col_name, "type": col_type})
        
        if not columns_to_add:
            return True, column_mapping
            
        # Ajouter les colonnes √† la table
        url = f"{client.base_url}/docs/{client.doc_id}/tables/{table_id}/columns"
        payload = {"columns": columns_to_add}
        
        log(f"  Ajout de {len(columns_to_add)} colonnes √† la table {table_id}")
        for col in columns_to_add:
            log_verbose(f"  - Ajout de la colonne '{col['id']}' (type: {col['type']})")
            
        response = requests.post(url, headers=client.headers, json=payload)
        
        if response.status_code == 200:
            log(f"  {len(columns_to_add)} colonnes ajout√©es avec succ√®s √† la table {table_id}")
            
            # V√©rifier que les colonnes ont bien √©t√© ajout√©es
            verify_url = f"{client.base_url}/docs/{client.doc_id}/tables/{table_id}/columns"
            verify_response = requests.get(verify_url, headers=client.headers)
            
            if verify_response.status_code == 200:
                columns_data = verify_response.json()
                existing_column_ids = set()
                
                if "columns" in columns_data:
                    for col in columns_data["columns"]:
                        existing_column_ids.add(col.get("id"))
                
                # V√©rifier quelles colonnes ont bien √©t√© ajout√©es
                all_added = True
                for col in columns_to_add:
                    if col["id"] not in existing_column_ids:
                        log_error(f"  Colonne '{col['id']}' n'a pas √©t√© ajout√©e")
                        all_added = False
                
                return all_added, column_mapping
            else:
                log_error(f"  Erreur lors de la v√©rification des colonnes: {verify_response.status_code} - {verify_response.text}")
                return False, column_mapping
        else:
            log_error(f"  Erreur lors de l'ajout des colonnes: {response.status_code} - {response.text}")
            log_error(f"  D√©tails: {response.text}")
            return False, column_mapping
            
    except Exception as e:
        log_error(f"  Erreur lors de l'ajout des colonnes: {str(e)}")
        import traceback
        traceback.print_exc()
        return False, column_mapping

def add_id_columns_based_on_annotations(client, table_id, annotations):
    """
    Ajoute des colonnes pour les IDs des annotations bas√©es sur leur label
    """
    columns_to_add = []
    
    for annotation in annotations:
        if "label" not in annotation or "id" not in annotation:
            continue
            
        original_label = annotation["label"]
        if original_label.startswith("annotation_"):
            normalized_label = normalize_column_name(original_label[11:])
        else:
            normalized_label = normalize_column_name(original_label)
            
        id_column = f"{normalized_label}_id"
        columns_to_add.append({"id": id_column, "type": "Text"})
    
    if columns_to_add:
        # V√©rifier les colonnes existantes pour √©viter des doublons
        url = f"{client.base_url}/docs/{client.doc_id}/tables/{table_id}/columns"
        response = requests.get(url, headers=client.headers)
        
        if response.status_code == 200:
            columns_data = response.json()
            existing_column_ids = set()
            
            if "columns" in columns_data:
                for col in columns_data["columns"]:
                    existing_column_ids.add(col.get("id"))
            
            # Filtrer pour n'ajouter que les colonnes manquantes
            columns_to_add = [col for col in columns_to_add if col["id"] not in existing_column_ids]
        
        if columns_to_add:
            url = f"{client.base_url}/docs/{client.doc_id}/tables/{table_id}/columns"
            payload = {"columns": columns_to_add}
            response = requests.post(url, headers=client.headers, json=payload)
            
            if response.status_code != 200:
                log_error(f"Erreur lors de l'ajout des colonnes d'ID: {response.text}")
            else:
                log(f"Colonnes d'ID ajout√©es avec succ√®s: {', '.join(col['id'] for col in columns_to_add)}")
    
    return [col["id"] for col in columns_to_add]


# Classe pour g√©rer les op√©rations avec l'API Grist
class GristClient:
    def __init__(self, base_url, api_key, doc_id=None):
        self.base_url = base_url.rstrip('/')  # Enlever le / final s'il y en a un
        self.api_key = api_key
        self.doc_id = doc_id
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        log(f"Initialisation du client Grist avec l'URL de base: {self.base_url}")

    def set_doc_id(self, doc_id):
        self.doc_id = doc_id

    def table_exists(self, table_id):
        """
        V√©rifie si une table existe dans le document Grist.
        """
        try:
            tables_data = self.list_tables()

            # V√©rification de la structure de tables_data
            if isinstance(tables_data, dict) and 'tables' in tables_data:
                tables = tables_data['tables']
            elif isinstance(tables_data, list):
                tables = tables_data
            else:
                log_verbose(f"Structure inattendue de donn√©es de tables: {type(tables_data)}")
                return None

            # Recherche case-insensitive
            for table in tables:
                if isinstance(table, dict) and table.get('id', '').lower() == table_id.lower():
                    log_verbose(f"Table {table_id} trouv√©e avec l'ID {table.get('id')}")
                    return table

            log_verbose(f"Table {table_id} non trouv√©e")
            return None

        except Exception as e:
            log_error(f"Erreur lors de la recherche de la table {table_id}: {e}")
            return None

    def get_existing_dossier_numbers(self, table_id):
        if not self.doc_id:
            raise ValueError("Document ID is required")

        url = f"{self.base_url}/docs/{self.doc_id}/tables/{table_id}/records"
        log_verbose(f"R√©cup√©ration des enregistrements existants depuis {url}")

        response = requests.get(url, headers=self.headers)
        if response.status_code == 200:
            data = response.json()
            
            log_verbose(f"Nombre total d'enregistrements r√©cup√©r√©s: {len(data.get('records', []))}")

            # Chercher les enregistrements avec dossier_number ou number
            dossier_dict = {}
            if 'records' in data and isinstance(data['records'], list):
                for record in data['records']:
                    if isinstance(record, dict) and 'fields' in record and isinstance(record['fields'], dict):
                        record_id = record.get('id')
                        fields = record.get('fields', {})
                        
                        # V√©rifier si dossier_number ou number est pr√©sent
                        dossier_num = None
                        if 'dossier_number' in fields and fields['dossier_number']:
                            dossier_num = fields['dossier_number']
                            dossier_dict[str(dossier_num)] = record_id
                        elif 'number' in fields and fields['number']:
                            dossier_num = fields['number']
                            dossier_dict[str(dossier_num)] = record_id

            log(f"Nombre de dossiers existants identifi√©s: {len(dossier_dict)}")
            return dossier_dict
        else:
            log_error(f"Erreur lors de la r√©cup√©ration des enregistrements existants: {response.status_code} - {response.text}")
            return {}

    def upsert_dossier_in_grist(self, table_id, row_dict):
        """
        Ins√®re ou met √† jour un dossier dans une table Grist, en filtrant les champs probl√©matiques.
        """
        # Log des champs avant filtrage
        log_verbose(f"Champs dans row_dict avant filtrage: {list(row_dict.keys())}")
        log_verbose(f"Pr√©sence de 'label_names': {'label_names' in row_dict}")
        log_verbose(f"Pr√©sence de 'labels_json': {'labels_json' in row_dict}")
    
        if 'label_names' in row_dict:
            log_verbose(f"Valeur de 'label_names': {row_dict['label_names']}")
        if 'labels_json' in row_dict:
            log_verbose(f"Valeur de 'labels_json': {row_dict['labels_json']}")
            if not self.doc_id:
                raise ValueError("Document ID is required")

        
        # V√©rifier si nous avons le num√©ro de dossier
        dossier_number = row_dict.get("dossier_number") or row_dict.get("number")

        if not dossier_number:
            log_error("dossier_number ou number manquant dans les donn√©es")
            log_verbose(f"Donn√©es disponibles: {row_dict.keys()}")
            return False

        # Convertir le num√©ro de dossier en cha√Æne pour les comparaisons
        dossier_number_str = str(dossier_number)

        # R√©cup√©ration des dossiers existants pour v√©rifier si on doit faire un update ou un insert
        log_verbose(f"R√©cup√©ration des dossiers existants pour la table {table_id}...")
        existing_records = self.get_existing_dossier_numbers(table_id)
        log_verbose(f"Dossiers existants trouv√©s: {len(existing_records)}")

        url = f"{self.base_url}/docs/{self.doc_id}/tables/{table_id}/records"

        # S'assurer que le dictionnaire est format√© correctement pour l'API Grist
        # Grist attend des champs sous la forme {"fields": {...}}
        formatted_row = {"fields": row_dict} if "fields" not in row_dict else row_dict

        log_verbose(f"Recherche du dossier {dossier_number_str} dans les enregistrements existants...")
        if dossier_number_str in existing_records:
            # Mise √† jour de l'enregistrement existant
            record_id = existing_records[dossier_number_str]
            log_verbose(f"Dossier {dossier_number_str} trouv√© avec ID {record_id}, mise √† jour...")
            update_payload = {"records": [{"id": record_id, "fields": formatted_row["fields"]}]}
            response = requests.patch(url, headers=self.headers, json=update_payload)
        else:
            # Cr√©ation d'un nouvel enregistrement
            log_verbose(f"Dossier {dossier_number_str} non trouv√©, cr√©ation d'un nouvel enregistrement...")
            create_payload = {"records": [formatted_row]}
            response = requests.post(url, headers=self.headers, json=create_payload)

        if response.status_code in [200, 201]:
            return True
        else:
            log_error(f"Erreur UPSERT pour {dossier_number_str}: {response.status_code} - {response.text}")
            return False

    def list_documents(self):
        url = f"{self.base_url}/docs"
        log_verbose(f"GET {url}")
        response = requests.get(url, headers=self.headers)
        if response.status_code != 200:
            log_error(f"Erreur {response.status_code}: {response.text}")
            response.raise_for_status()

        data = response.json()
        return data

    def get_document_info(self):
        if not self.doc_id:
            raise ValueError("Document ID is required")
        url = f"{self.base_url}/docs/{self.doc_id}"
        log_verbose(f"GET {url}")
        response = requests.get(url, headers=self.headers)
        if response.status_code != 200:
            log_error(f"Erreur {response.status_code}: {response.text}")
            response.raise_for_status()

        data = response.json()
        return data
    
    def list_tables(self):
        if not self.doc_id:
            raise ValueError("Document ID is required")

        url = f"{self.base_url}/docs/{self.doc_id}/tables"
        log_verbose(f"GET {url}")
        response = requests.get(url, headers=self.headers)
        if response.status_code != 200:
            log_error(f"Erreur {response.status_code}: {response.text}")
            response.raise_for_status()

        data = response.json()
        return data
    
    def create_table(self, table_id, columns):
        if not self.doc_id:
            raise ValueError("Document ID is required")

        url = f"{self.base_url}/docs/{self.doc_id}/tables"
        data = {
            "tables": [
                {
                    "id": table_id,
                    "columns": columns
                }
            ]
        }
        log(f"Cr√©ation de la table {table_id}")

        for col in columns:
            if 'id' not in col or not col['id']:
                raise ValueError(f"Column missing id: {col}")
            if 'type' not in col or not col['type']:
                raise ValueError(f"Invalid column id '{col['id']}'. Must start with a letter and contain only letters, numbers, and underscores.")

        response = requests.post(url, headers=self.headers, json=data)
        if response.status_code != 200:
            log_error(f"Erreur {response.status_code}: {response.text}")
            response.raise_for_status()

        result = response.json()
        return result

    def create_or_clear_grist_tables(self, demarche_number, column_types):
        """
        Cr√©e ou met √† jour les tables Grist pour une d√©marche.
        """
        try:
            # R√©cup√©rer les indicateurs de pr√©sence
            has_repetable_blocks = column_types.get("has_repetable_blocks", False)
            has_carto_fields = column_types.get("has_carto_fields", False)
            
            # FILTRAGE EXPLICITE DES COLONNES PROBL√âMATIQUES
            # Retirer toutes les colonnes qui pourraient correspondre √† HeaderSectionChamp et ExplicationChamp
            filtered_champ_columns = column_types.get("champs", [])
            for col in column_types.get("champs", []):
                col_id = col.get("id", "").lower()
            # Remplacer les colonnes originales par les colonnes filtr√©es
            column_types["champs"] = filtered_champ_columns
            
            # Filtrage similaire pour les colonnes d'annotations
            filtered_annotation_columns = column_types.get("annotations", [])
            for col in column_types.get("annotations", []):
                col_id = col.get("id", "").lower()
            # Remplacer les colonnes originales par les colonnes filtr√©es
            column_types["annotations"] = filtered_annotation_columns
            
            # D√©finir les IDs de tables
            dossier_table_id = f"Demarche_{demarche_number}_dossiers"
            champ_table_id = f"Demarche_{demarche_number}_champs"
            annotation_table_id = f"Demarche_{demarche_number}_annotations"
            repetable_table_id = f"Demarche_{demarche_number}_repetable_rows" if has_repetable_blocks else None

            # R√©cup√©rer les tables existantes
            existing_tables_response = self.list_tables()
            existing_tables = existing_tables_response.get('tables', [])

            # Rechercher les tables existantes (dossiers, champs, annotations, r√©p√©tables)
            dossier_table = None
            champ_table = None
            annotation_table = None
            repetable_table = None

            for table in existing_tables:
                if isinstance(table, dict):
                    table_id = table.get('id', '').lower()
                    if table_id == dossier_table_id.lower():
                        dossier_table = table
                        dossier_table_id = table.get('id')
                        log(f"Table dossiers existante trouv√©e avec l'ID {dossier_table_id}")
                    elif table_id == champ_table_id.lower():
                        champ_table = table
                        champ_table_id = table.get('id')
                        log(f"Table champs existante trouv√©e avec l'ID {champ_table_id}")
                    elif table_id == annotation_table_id.lower():
                        annotation_table = table
                        annotation_table_id = table.get('id')
                        log(f"Table annotations existante trouv√©e avec l'ID {annotation_table_id}")
                    elif repetable_table_id and table_id == repetable_table_id.lower():
                        repetable_table = table
                        repetable_table_id = table.get('id')
                        log(f"Table r√©p√©tables existante trouv√©e avec l'ID {repetable_table_id}")

            # Cr√©er la table des dossiers si elle n'existe pas
            if not dossier_table:
                log(f"Cr√©ation de la table {dossier_table_id}")
                dossier_table_result = self.create_table(dossier_table_id, column_types["dossier"])
                dossier_table = dossier_table_result['tables'][0]
                dossier_table_id = dossier_table.get('id')

            # Cr√©er la table des champs si elle n'existe pas
            if not champ_table:
                log(f"Cr√©ation de la table {champ_table_id}")
                champ_table_result = self.create_table(champ_table_id, column_types["champs"])
                champ_table = champ_table_result['tables'][0]
                champ_table_id = champ_table.get('id')
                
            # Cr√©er la table des annotations si elle n'existe pas
            if not annotation_table:
                log(f"Cr√©ation de la table {annotation_table_id}")
                annotation_table_result = self.create_table(annotation_table_id, column_types["annotations"])
                annotation_table = annotation_table_result['tables'][0]
                annotation_table_id = annotation_table.get('id')

            # Cr√©er la table des blocs r√©p√©tables seulement si n√©cessaire
            if has_repetable_blocks and repetable_table_id and not repetable_table and "repetable_rows" in column_types:
                log(f"Cr√©ation de la table {repetable_table_id}")
                # Commencer avec seulement les colonnes de base pour √©viter des erreurs
                base_columns = [
                    {"id": "dossier_number", "type": "Int"},
                    {"id": "block_label", "type": "Text"},
                    {"id": "block_row_index", "type": "Int"},
                    {"id": "block_row_id", "type": "Text"}
                ]
                repetable_table_result = self.create_table(repetable_table_id, base_columns)
                repetable_table = repetable_table_result['tables'][0]
                repetable_table_id = repetable_table.get('id')
                
                # Ajouter les colonnes suppl√©mentaires une par une
                remaining_columns = []
                for col in column_types["repetable_rows"]:
                    if col["id"] not in ["dossier_number", "block_label", "block_row_index", "block_row_id"]:
                        
                        remaining_columns.append(col)
                
                # Ajouter des colonnes cartographiques sp√©cifiques seulement si des champs carto sont pr√©sents
                if has_carto_fields:
                    geo_columns = [
                        {"id": "geo_id", "type": "Text"},
                        {"id": "geo_source", "type": "Text"},
                        {"id": "geo_description", "type": "Text"},
                        {"id": "geo_type", "type": "Text"},
                        {"id": "geo_coordinates", "type": "Text"},
                        {"id": "geo_wkt", "type": "Text"},
                        {"id": "geo_commune", "type": "Text"},
                        {"id": "geo_numero", "type": "Text"},
                        {"id": "geo_section", "type": "Text"},
                        {"id": "geo_prefixe", "type": "Text"},
                        {"id": "geo_surface", "type": "Numeric"}
                    ]
                    
                    # Ajouter chaque colonne g√©ographique si elle n'est pas d√©j√† dans remaining_columns
                    for geo_col in geo_columns:
                        if not any(col["id"] == geo_col["id"] for col in remaining_columns):
                            remaining_columns.append(geo_col)
                
                if remaining_columns:
                    log(f"Ajout de {len(remaining_columns)} colonnes suppl√©mentaires √† la table des blocs r√©p√©tables...")
                    try:
                        url = f"{self.base_url}/docs/{self.doc_id}/tables/{repetable_table_id}/columns"
                        add_columns_payload = {"columns": remaining_columns}
                        response = requests.post(url, headers=self.headers, json=add_columns_payload)
                        
                        if response.status_code != 200:
                            log_error(f"Erreur lors de l'ajout des colonnes: {response.text}")
                        else:
                            log("Colonnes ajout√©es avec succ√®s")
                    except Exception as e:
                        log_error(f"Erreur lors de l'ajout des colonnes: {str(e)}")
                        import traceback
                        traceback.print_exc()
                        
            elif has_repetable_blocks and repetable_table and repetable_table_id and "repetable_rows" in column_types:
                # La table existe d√©j√†, v√©rifier que toutes les colonnes sont pr√©sentes
                try:
                    url = f"{self.base_url}/docs/{self.doc_id}/tables/{repetable_table_id}/columns"
                    response = requests.get(url, headers=self.headers)
                    
                    if response.status_code == 200:
                        columns_data = response.json()
                        existing_column_ids = set()
                        
                        if "columns" in columns_data:
                            for col in columns_data["columns"]:
                                existing_column_ids.add(col.get("id"))
                        
                        # Trouver les colonnes manquantes
                        missing_columns = []
                        for col in column_types["repetable_rows"]:  
                            if col["id"] not in existing_column_ids:
                                missing_columns.append(col)
                        
                        # Ajouter des colonnes cartographiques sp√©cifiques seulement si des champs carto sont pr√©sents
                        if has_carto_fields:
                            geo_columns = [
                                {"id": "geo_id", "type": "Text"},
                                {"id": "geo_source", "type": "Text"},
                                {"id": "geo_description", "type": "Text"},
                                {"id": "geo_type", "type": "Text"},
                                {"id": "geo_coordinates", "type": "Text"},
                                {"id": "geo_wkt", "type": "Text"},
                                {"id": "geo_commune", "type": "Text"},
                                {"id": "geo_numero", "type": "Text"},
                                {"id": "geo_section", "type": "Text"},
                                {"id": "geo_prefixe", "type": "Text"},
                                {"id": "geo_surface", "type": "Numeric"}
                            ]
                            
                            # Ajouter chaque colonne g√©ographique si elle n'est pas d√©j√† pr√©sente
                            for geo_col in geo_columns:
                                if geo_col["id"] not in existing_column_ids and not any(col["id"] == geo_col["id"] for col in missing_columns):
                                    missing_columns.append(geo_col)
                        
                        if missing_columns:
                            log(f"Ajout de {len(missing_columns)} colonnes manquantes √† la table des blocs r√©p√©tables...")
                            add_columns_url = f"{self.base_url}/docs/{self.doc_id}/tables/{repetable_table_id}/columns"
                            add_columns_payload = {"columns": missing_columns}
                            add_response = requests.post(add_columns_url, headers=self.headers, json=add_columns_payload)
                            
                            if add_response.status_code != 200:
                                log_error(f"Erreur lors de l'ajout des colonnes: {add_response.text}")
                            else:
                                log("Colonnes ajout√©es avec succ√®s")
                    else:
                        log_error(f"Erreur lors de la r√©cup√©ration des colonnes: {response.text}")
                except Exception as e:
                    log_error(f"Erreur lors de la v√©rification des colonnes: {str(e)}")
                    import traceback
                    traceback.print_exc()

            # Retourner les IDs des tables
            return {
                "dossier_table_id": dossier_table_id,
                "champ_table_id": champ_table_id,
                "annotation_table_id": annotation_table_id,
                "repetable_table_id": repetable_table_id
            }

        except Exception as e:
            log_error(f"Erreur lors de la gestion des tables Grist: {e}")
            import traceback
            traceback.print_exc()
            raise

    # 2. Ensuite, modifiez la m√©thode upsert_multiple_dossiers_in_grist de la classe GristClient

    def upsert_multiple_dossiers_in_grist(self, table_id, dossiers_list):
        """
        Ins√®re ou met √† jour plusieurs dossiers en une seule requ√™te.
        Version corrig√©e avec gestion appropri√©e des succ√®s/√©checs.
        """
        if not self.doc_id:
            raise ValueError("Document ID is required")
        
        # R√©cup√©rer tous les enregistrements existants en une seule requ√™te
        existing_records = self.get_existing_dossier_numbers(table_id)
        log_verbose(f"R√©cup√©ration de {len(existing_records)} enregistrements existants pour traitement par lot")
        
        # R√©cup√©rer les colonnes existantes une seule fois
        existing_columns = set()
        try:
            url = f"{self.base_url}/docs/{self.doc_id}/tables/{table_id}/columns"
            response = requests.get(url, headers=self.headers)
            
            if response.status_code == 200:
                columns_data = response.json()
                if "columns" in columns_data:
                    for col in columns_data["columns"]:
                        existing_columns.add(col.get("id"))
                
                log_verbose(f"Colonnes existantes dans la table {table_id}: {len(existing_columns)}")
        except Exception as e:
            log_error(f"Erreur lors de la r√©cup√©ration des colonnes: {str(e)}")
        
        # Pr√©parer les listes pour les op√©rations de cr√©ation et de mise √† jour
        to_create = []
        to_update = []
        
        for row_dict in dossiers_list:
            # Filtrer les colonnes qui existent dans la table
            filtered_row_dict = {}
            for key, value in row_dict.items():
                if not existing_columns or key in existing_columns or key == "dossier_number":
                    filtered_row_dict[key] = value
            
            # Obtenir le num√©ro de dossier
            dossier_number = filtered_row_dict.get("dossier_number") or filtered_row_dict.get("number")
            if not dossier_number:
                log_error("dossier_number ou number manquant dans les donn√©es")
                continue
            
            dossier_number_str = str(dossier_number)
            
            if dossier_number_str in existing_records:
                # Mise √† jour d'un enregistrement existant
                record_id = existing_records[dossier_number_str]
                to_update.append({"id": record_id, "fields": filtered_row_dict})
            else:
                # Cr√©ation d'un nouvel enregistrement
                to_create.append({"fields": filtered_row_dict})
        
        # Variables pour suivre les succ√®s
        total_success = 0
        total_errors = 0
        
        # Traitement des mises √† jour
        if to_update:
            # Normaliser tous les enregistrements pour qu'ils aient les m√™mes champs
            all_update_keys = set()
            for record in to_update:
                all_update_keys.update(record["fields"].keys())
            
            normalized_updates = []
            for record in to_update:
                normalized_fields = {}
                for key in all_update_keys:
                    normalized_fields[key] = record["fields"].get(key, None)
                normalized_updates.append({"id": record["id"], "fields": normalized_fields})
            
            # Traitement sp√©cial pour la table des champs
            if "champ" in table_id.lower():
                log(f"Table des champs d√©tect√©e - traitement individuel de {len(normalized_updates)} mises √† jour...")
                
                update_success = 0
                update_errors = 0
                
                for record in normalized_updates:
                    record_id = record["id"]
                    fields = record["fields"]
                    
                    update_url = f"{self.base_url}/docs/{self.doc_id}/tables/{table_id}/records"
                    update_payload = {"records": [{"id": record_id, "fields": fields}]}
                    update_response = requests.patch(update_url, headers=self.headers, json=update_payload)
                    
                    if update_response.status_code in [200, 201]:
                        update_success += 1
                    else:
                        update_errors += 1
                        log_error(f"Erreur lors de la mise √† jour de l'enregistrement {record_id}: {update_response.status_code}")
                
                log(f"Mises √† jour individuelles pour la table des champs: {update_success} succ√®s, {update_errors} √©checs")
                total_success += update_success
                total_errors += update_errors
            else:
                # Mise √† jour par lot pour les autres tables
                update_url = f"{self.base_url}/docs/{self.doc_id}/tables/{table_id}/records"
                update_payload = {"records": normalized_updates}
                update_response = requests.patch(update_url, headers=self.headers, json=update_payload)
                
                if update_response.status_code in [200, 201]:
                    log(f"Mise √† jour par lot: {len(normalized_updates)} enregistrements mis √† jour avec succ√®s")
                    total_success += len(normalized_updates)
                else:
                    log_error(f"Erreur lors de la mise √† jour par lot: {update_response.status_code} - {update_response.text}")
                    
                    # Fallback: essayer individuellement
                    log("Tentative de mise √† jour individuelle...")
                    update_success = 0
                    for individual_record in normalized_updates:
                        individual_payload = {"records": [individual_record]}
                        individual_response = requests.patch(update_url, headers=self.headers, json=individual_payload)
                        
                        if individual_response.status_code in [200, 201]:
                            update_success += 1
                        else:
                            total_errors += 1
                            log_error(f"√âchec individuel pour {individual_record['id']}")
                    
                    total_success += update_success
                    log(f"Mise √† jour individuelle: {update_success}/{len(normalized_updates)} succ√®s")
        
        # Traitement des cr√©ations
        if to_create:
            # Normaliser tous les enregistrements de cr√©ation
            all_create_keys = set()
            for record in to_create:
                all_create_keys.update(record["fields"].keys())
            
            normalized_creations = []
            for record in to_create:
                normalized_fields = {}
                for key in all_create_keys:
                    normalized_fields[key] = record["fields"].get(key, None)
                normalized_creations.append({"fields": normalized_fields})
            
            create_url = f"{self.base_url}/docs/{self.doc_id}/tables/{table_id}/records"
            create_payload = {"records": normalized_creations}
            create_response = requests.post(create_url, headers=self.headers, json=create_payload)
            
            if create_response.status_code in [200, 201]:
                log(f"Cr√©ation par lot: {len(normalized_creations)} enregistrements cr√©√©s avec succ√®s")
                total_success += len(normalized_creations)
            else:
                log_error(f"Erreur lors de la cr√©ation par lot: {create_response.status_code} - {create_response.text}")
                total_errors += len(normalized_creations)
        
        # Retourner le succ√®s global
        success = total_success > 0 and total_errors == 0
        
        # Log du r√©sum√©
        if total_success > 0 or total_errors > 0:
            log(f"R√©sum√© upsert table {table_id}: {total_success} succ√®s, {total_errors} √©checs")
        
        return success
    
def process_dossier_for_grist(client, dossier_data, table_ids, column_types, problematic_ids=None):
    """
    Traite un dossier pour l'ins√©rer dans les tables Grist.
    """
    try:
        # V√©rifier si des blocs r√©p√©tables doivent √™tre trait√©s
        has_repetable_blocks = column_types.get("has_repetable_blocks", False)
        
        # Extraire les donn√©es √† plat du dossier, en excluant les blocs r√©p√©tables
        # pour √©viter la duplication avec la table des blocs r√©p√©tables
        exclude_repetition = has_repetable_blocks  # N'exclure que si on va les traiter s√©par√©ment
        flat_data = dossier_to_flat_data(dossier_data, exclude_repetition_champs=exclude_repetition, problematic_ids=problematic_ids)
        dossier_info = flat_data["dossier"]
        dossier_number = dossier_info["dossier_number"]
        
        # Convertir le num√©ro de dossier en cha√Æne pour toutes les comparaisons
        dossier_number_str = str(dossier_number)

        # V√©rifier si le dossier a √©t√© supprim√© par l'usager
        if "dateSuppressionParUsager" in dossier_data and dossier_data["dateSuppressionParUsager"]:
            # Option 1: Marquer le dossier comme supprim√© dans Grist
            log(f"Dossier {dossier_number} marqu√© comme supprim√© par l'usager le {dossier_data['dateSuppressionParUsager']}")
            dossier_info["supprime_par_usager"] = True
            dossier_info["date_suppression"] = dossier_data["dateSuppressionParUsager"]

        # --- PARTIE 1: TRAITEMENT DE LA TABLE DOSSIERS ---
        
        # Pr√©parer l'enregistrement pour la table des dossiers
        dossier_record = {}
        for column in column_types["dossier"]:
            field_id = column["id"]
            field_type = column["type"]

            if field_id in dossier_info:
                value = dossier_info[field_id]
            elif "dossier_" + field_id in dossier_info:
                value = dossier_info["dossier_" + field_id]
            else:
                continue

            dossier_record[field_id] = format_value_for_grist(value, field_type)

        # Ajouter explicitement l'ID du dossier si disponible
        if "dossier_id" in dossier_info:
            dossier_record["dossier_id"] = dossier_info["dossier_id"]

        # V√©rifier que dossier_number est pr√©sent dans l'enregistrement
        if "number" not in dossier_record:
            dossier_record["number"] = dossier_number
        
        # Traitement des labels
        # V√©rifier si les labels sont pr√©sents dans dossier_data
        if "labels" not in dossier_data or not dossier_data.get("labels"):
        # Si les labels ne sont pas pr√©sents, essayer de les r√©cup√©rer s√©par√©ment
            log_verbose(f"Labels non trouv√©s dans dossier_data, tentative de r√©cup√©ration s√©par√©e...")
            labels = get_dossier_labels(dossier_number)

            if labels:
                log_verbose(f"{len(labels)} labels r√©cup√©r√©s s√©par√©ment")

                # Cr√©er label_names
                label_names = [label.get("name", "") for label in labels if label.get("name")]
                dossier_record["label_names"] = ", ".join(label_names) if label_names else ""

                # Cr√©er labels_json
                labels_with_colors = [
                    {
                        "id": label.get("id", ""),
                        "name": label.get("name", ""),
                        "color": label.get("color", "")
                    }
                    for label in labels if label.get("name") and label.get("color")
                ]
                if labels_with_colors:
                    import json
                    dossier_record["labels_json"] = json_module.dumps(labels_with_colors, ensure_ascii=False)
                    log_verbose(f"label_names ajout√©: {dossier_record['label_names']}")
                    log_verbose(f"labels_json ajout√©: {dossier_record['labels_json']}")
                else:
                    dossier_record["labels_json"] = ""

        # Utiliser upsert pour la table des dossiers
        dossier_table_id = table_ids["dossier_table_id"]
        success_dossier = client.upsert_dossier_in_grist(dossier_table_id, dossier_record)
        if not success_dossier:
            log_error(f"√âchec de mise √† jour/insertion du dossier {dossier_number} dans la table {dossier_table_id}")
            return False
        
        # --- PARTIE 2: GESTION DES COLONNES MANQUANTES POUR LA TABLE CHAMPS ---
        
        # R√©cup√©rer les colonnes existantes dans la table des champs
        champ_table_id = table_ids["champ_table_id"]
        url = f"{client.base_url}/docs/{client.doc_id}/tables/{champ_table_id}/columns"
        response = requests.get(url, headers=client.headers)
        existing_columns = set()
        if response.status_code == 200:
            columns_data = response.json()
            if "columns" in columns_data:
                existing_columns = {col["id"] for col in columns_data["columns"]}

        # Collecter les colonnes manquantes
        missing_columns = []

        for champ in flat_data["champs"]:
            # Ignorer les champs probl√©matiques
            if champ["type"] in ["HeaderSectionChamp", "ExplicationChamp"]:
                continue
        
        champ_label = normalize_column_name(champ["label"])

        # V√©rifier si la colonne existe d√©j√†
        if champ_label not in existing_columns:
            log(f"  Colonne {champ_label} non trouv√©e dans le sch√©ma, sera ajout√©e")
            missing_columns.append(champ_label)

            # Mettre √† jour l'ensemble des colonnes existantes
            existing_columns.update(missing_columns)

        # --- PARTIE 3: TRAITEMENT DE LA TABLE CHAMPS ---

        # R√©cup√©rer les colonnes existantes dans la table des champs
        champ_table_id = table_ids["champ_table_id"]
        url = f"{client.base_url}/docs/{client.doc_id}/tables/{champ_table_id}/columns"
        response = requests.get(url, headers=client.headers)
        existing_columns = set()
        if response.status_code == 200:
            columns_data = response.json()
            if "columns" in columns_data:
                existing_columns = {col["id"] for col in columns_data["columns"]}

        # Collecter les colonnes manquantes
        missing_columns = []

        # Parcourir tous les champs pour d√©tecter les colonnes manquantes
        for champ in flat_data["champs"]:
            # Ignorer les champs probl√©matiques
            if champ["type"] in ["HeaderSectionChamp", "ExplicationChamp"]:
                continue
            
            champ_label = normalize_column_name(champ["label"])

            # V√©rifier si la colonne existe d√©j√†
            if champ_label not in existing_columns:
                log(f"  Colonne {champ_label} non trouv√©e dans le sch√©ma, sera ajout√©e")
                missing_columns.append(champ["label"])  # Utiliser le label original ici

        # Ajouter les colonnes manquantes si n√©cessaire
        if missing_columns:
            success, column_mapping = add_missing_columns_to_table(client, champ_table_id, missing_columns, column_types)
            
            if success:
                # Mettre √† jour l'ensemble des colonnes existantes avec les noms normalis√©s
                existing_columns.update(column_mapping.values())
            else:
                log_error("√âchec de l'ajout des colonnes, certaines donn√©es pourraient √™tre omises")

        # Pr√©parer un enregistrement unique pour la table des champs
        champs_record = {"dossier_number": dossier_number}
        champ_column_types = {col["id"]: col["type"] for col in column_types["champs"]}

        # Collecter les IDs des champs pour les stocker
        champ_ids = []
        for champ in flat_data["champs"]:
            if champ.get("id"):
                champ_ids.append(str(champ["id"]))
        if champ_ids:
            champs_record["champ_id"] = "_".join(champ_ids)


        # Agr√©ger tous les champs dans un seul enregistrement
        for champ in flat_data["champs"]:
            # Ignorer les champs de type HeaderSectionChamp et ExplicationChamp
            if champ["type"] in ["HeaderSectionChamp", "ExplicationChamp"]:
                continue
                
            # Normaliser le label pour obtenir le nom de colonne
            original_label = champ["label"]
            champ_label = normalize_column_name(original_label)
            
            # V√©rifier si la colonne existe dans le sch√©ma Grist
            if champ_label not in existing_columns:
                # M√™me si on a ajout√© les colonnes manquantes, v√©rifier √† nouveau
                log_verbose(f"  Colonne {champ_label} toujours non disponible, ignor√©e")
                continue

            value = champ.get("value", "")
            # Pour les types complexes, utiliser la repr√©sentation JSON si disponible
            if champ["type"] in ["CarteChamp", "AddressChamp", "SiretChamp"] and champ.get("json_value"):
                try:
                    value = json_module.dumps(champ["json_value"], ensure_ascii=False)
                except (TypeError, ValueError):
                    value = str(champ["json_value"])
            
            # D√©terminer le type de colonne (par d√©faut Text)
            column_type = champ_column_types.get(champ_label, "Text")
            champs_record[champ_label] = format_value_for_grist(value, column_type)

        # Traitement de la table des champs
        existing_champs = client.get_existing_dossier_numbers(champ_table_id)

        success_champs = False
        if dossier_number_str in existing_champs:
            # Mise √† jour d'un enregistrement existant
            champ_record_id = existing_champs[dossier_number_str]
            update_payload = {"records": [{"id": champ_record_id, "fields": champs_record}]}
            url = f"{client.base_url}/docs/{client.doc_id}/tables/{champ_table_id}/records"
            response = requests.patch(url, headers=client.headers, json=update_payload)
            success_champs = response.status_code in [200, 201]
        else:
            # Cr√©ation d'un nouvel enregistrement
            create_payload = {"records": [{"fields": champs_record}]}
            url = f"{client.base_url}/docs/{client.doc_id}/tables/{champ_table_id}/records"
            response = requests.post(url, headers=client.headers, json=create_payload)
            success_champs = response.status_code in [200, 201]

        if not success_champs:
            log_error(f"Erreur lors du traitement des champs pour {dossier_number_str}: {response.text}")
            return False
        
        # --- PARTIE 4: GESTION DES COLONNES MANQUANTES POUR LA TABLE ANNOTATIONS ---

        # R√©cup√©rer les colonnes existantes dans la table des annotations
        annotation_table_id = table_ids["annotation_table_id"]

        url = f"{client.base_url}/docs/{client.doc_id}/tables/{annotation_table_id}/columns"
        response = requests.get(url, headers=client.headers)

        existing_annotation_columns = set()
        if response.status_code == 200:
            columns_data = response.json()
            if "columns" in columns_data:
                existing_annotation_columns = {col["id"] for col in columns_data["columns"]}

        # Collecter les colonnes d'annotations manquantes
        missing_annotation_columns = []

        for annotation in flat_data["annotations"]:
            # Ignorer les champs probl√©matiques
            if annotation["type"] in ["HeaderSectionChamp", "ExplicationChamp"]:
                continue
                
            # Pour la table des annotations, enlever le pr√©fixe "annotation_"
            original_label = annotation["label"]
            if original_label.startswith("annotation_"):
                annotation_label = normalize_column_name(original_label[11:])  # enlever "annotation_"
            else:
                annotation_label = normalize_column_name(original_label)
            
            # V√©rifier si la colonne existe d√©j√†
            if annotation_label not in existing_annotation_columns:
                log(f"  Colonne d'annotation {annotation_label} non trouv√©e dans le sch√©ma, sera ajout√©e")
                missing_annotation_columns.append(annotation_label)

        # Ajouter les colonnes d'annotations manquantes si n√©cessaire
        if missing_annotation_columns:
            log(f"  Ajout de {len(missing_annotation_columns)} colonnes manquantes √† la table {annotation_table_id}")
            add_missing_columns_to_table(client, annotation_table_id, missing_annotation_columns, column_types)
            
            # Mettre √† jour l'ensemble des colonnes existantes
            existing_annotation_columns.update(missing_annotation_columns)

        # --- PARTIE 5: TRAITEMENT DE LA TABLE ANNOTATIONS ---

        # R√©cup√©rer les colonnes existantes dans la table des annotations
        annotation_table_id = table_ids["annotation_table_id"]

        # Ajouter dynamiquement les colonnes pour les IDs des annotations
        add_id_columns_based_on_annotations(client, annotation_table_id, flat_data["annotations"])

        # R√©cup√©rer √† nouveau les colonnes apr√®s l'ajout des colonnes d'ID
        url = f"{client.base_url}/docs/{client.doc_id}/tables/{annotation_table_id}/columns"
        response = requests.get(url, headers=client.headers)

        existing_annotation_columns = set()
        if response.status_code == 200:
            columns_data = response.json()
            if "columns" in columns_data:
                for col in columns_data["columns"]:
                    existing_annotation_columns.add(col.get("id"))

        # Pr√©parer un enregistrement unique pour la table des annotations
        annotations_record = {"dossier_number": dossier_number}
        annotation_column_types = {col["id"]: col["type"] for col in column_types["annotations"]}

        # Collecter les IDs des annotations pour les stocker
        annotation_ids = []
        for annotation in flat_data["annotations"]:
            if annotation.get("id"):
                annotation_ids.append(str(annotation["id"]))
        if annotation_ids:
            annotations_record["annotation_id"] = "_".join(annotation_ids)
        
        
        # Agr√©ger toutes les annotations dans un seul enregistrement
        for annotation in flat_data["annotations"]:
            # Ignorer les annotations probl√©matiques
            if annotation["type"] in ["HeaderSectionChamp", "ExplicationChamp"]:
                continue
                
            # Pour la table des annotations, enlever le pr√©fixe "annotation_"
            original_label = annotation["label"]
            if original_label.startswith("annotation_"):
                annotation_label = normalize_column_name(original_label[11:])  # enlever "annotation_"
            else:
                annotation_label = normalize_column_name(original_label)
            
            # V√©rifier si la colonne existe dans le sch√©ma (m√™me apr√®s ajout)
            if annotation_label not in existing_annotation_columns:
                log_verbose(f"  Colonne d'annotation {annotation_label} toujours non disponible, ignor√©e")
                continue
            
            value = annotation.get("value", "")
            # Pour les types complexes, utiliser la repr√©sentation JSON si disponible
            if annotation["type"] in ["CarteChamp", "AddressChamp", "SiretChamp"] and annotation.get("json_value"):
                try:
                    value = json_module.dumps(annotation["json_value"], ensure_ascii=False)
                except (TypeError, ValueError):
                    value = str(annotation["json_value"])
            
            # D√©terminer le type de colonne (par d√©faut Text)
            column_type = annotation_column_types.get(annotation_label, "Text")
            annotations_record[annotation_label] = format_value_for_grist(value, column_type)

            

        # Traitement de la table des annotations
        existing_annotations = client.get_existing_dossier_numbers(annotation_table_id)

        success_annotations = False
        if dossier_number_str in existing_annotations:
            # Mise √† jour d'un enregistrement existant
            annotation_record_id = existing_annotations[dossier_number_str]
            update_payload = {"records": [{"id": annotation_record_id, "fields": annotations_record}]}
            url = f"{client.base_url}/docs/{client.doc_id}/tables/{annotation_table_id}/records"
            response = requests.patch(url, headers=client.headers, json=update_payload)
            success_annotations = response.status_code in [200, 201]
        else:
            # Cr√©ation d'un nouvel enregistrement
            create_payload = {"records": [{"fields": annotations_record}]}
            url = f"{client.base_url}/docs/{client.doc_id}/tables/{annotation_table_id}/records"
            response = requests.post(url, headers=client.headers, json=create_payload)
            success_annotations = response.status_code in [200, 201]

        if not success_annotations:
            log_error(f"Erreur lors du traitement des annotations pour {dossier_number_str}: {response.text}")
            # Ne pas retourner d'erreur si l'enregistrement des annotations √©choue, continuer le traitement
            log_error("Continuation du traitement malgr√© l'√©chec des annotations")

        # --- PARTIE 6: TRAITEMENT DES BLOCS R√âP√âTABLES ---
        
        # Ne traiter les blocs r√©p√©tables que si:
        # 1. Des blocs r√©p√©tables ont √©t√© d√©tect√©s
        # 2. La table des blocs r√©p√©tables existe
        # 3. Les d√©finitions de colonnes pour les blocs r√©p√©tables existent
        if has_repetable_blocks and table_ids.get("repetable_table_id") and "repetable_rows" in column_types:
            repetable_table_id = table_ids["repetable_table_id"]
            repetable_column_types = column_types.get("repetable_rows", [])
            
            # Appeler le processeur de blocs r√©p√©tables
            try:
                import repetable_processor as rp
                success_count, error_count = rp.process_repetables_for_grist(
                    client, 
                    dossier_data,  # Passer les donn√©es brutes de l'API, pas les donn√©es aplaties
                    repetable_table_id, 
                    repetable_column_types
                )
                log(f"  Blocs r√©p√©tables trait√©s: {success_count} r√©ussis, {error_count} en √©chec")
            except Exception as e:
                log_error(f"  Erreur lors du traitement des blocs r√©p√©tables: {str(e)}")
                import traceback
                traceback.print_exc()
        elif has_repetable_blocks:
            log_verbose(f"  Blocs r√©p√©tables d√©tect√©s mais pas de table correspondante configur√©e")
        
        # Consid√©rer l'op√©ration comme r√©ussie
        return True

    except Exception as e:
        log_error(f"Erreur lors du traitement du dossier {dossier_number} pour Grist: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def process_demarche_for_grist(client, demarche_number):
    """
    Traite une d√©marche pour l'ins√©rer dans Grist.
    """
    try:
        # Initialiser des ensembles pour suivre les dossiers trait√©s avec succ√®s/√©chec
        successful_dossiers = set()
        failed_dossiers = set()
        
        # R√©cup√©rer les IDs des champs probl√©matiques
        problematic_descriptor_ids = get_problematic_descriptor_ids(demarche_number)
        log(f"Filtrage de {len(problematic_descriptor_ids)} descripteurs de type HeaderSectionChamp et ExplicationChamp")

        # V√©rifier que le document Grist existe
        try:
            doc_info = client.get_document_info()
            doc_name = doc_info.get("name", client.doc_id) if isinstance(doc_info, dict) else "Document ID " + client.doc_id
            log(f"Document Grist trouv√©: {doc_name}")
        except Exception as e:
            log_error(f"Erreur lors de la v√©rification du document Grist: {e}")
            return False

        # R√©cup√©rer les donn√©es de la d√©marche
        log(f"R√©cup√©ration des donn√©es de la d√©marche {demarche_number}...")
        demarche_data = get_demarche(demarche_number)
        if not demarche_data:
            log_error(f"Aucune donn√©e trouv√©e pour la d√©marche {demarche_number}")
            return False

        log(f"D√©marche trouv√©e: {demarche_data['title']}")

        # V√©rifier que des dossiers sont pr√©sents
        if "dossiers" not in demarche_data or "nodes" not in demarche_data["dossiers"]:
            log_error("Aucun dossier trouv√© dans la d√©marche")
            return False

        dossiers = demarche_data["dossiers"]["nodes"]
        total_dossiers = len(dossiers)
        if total_dossiers == 0:
            log_error("La d√©marche ne contient aucun dossier")
            return False

        log(f"Nombre de dossiers trouv√©s: {total_dossiers}")

        # R√©cup√©rer quelques dossiers pour analyse du sch√©ma
        sample_dossier_details = []
        max_sample_dossiers = min(3, total_dossiers)
        
        for i in range(max_sample_dossiers):
            sample_dossier_number = dossiers[i]["number"]
            log(f"R√©cup√©ration du dossier {sample_dossier_number} pour d√©tecter les types de colonnes... ({i+1}/{max_sample_dossiers})")
            sample_dossier = get_dossier(sample_dossier_number)
            if sample_dossier:
                sample_dossier_details.append(sample_dossier)
            else:
                log_error(f"Impossible de r√©cup√©rer le dossier {sample_dossier_number}")
                
        if not sample_dossier_details:
            log_error("Aucun dossier n'a pu √™tre r√©cup√©r√© pour l'analyse du sch√©ma")
            return False

        # D√©tecter les types de colonnes
        column_types = detect_column_types_from_multiple_dossiers(sample_dossier_details, problematic_ids=problematic_descriptor_ids)
        
        # R√©cup√©rer les indicateurs de pr√©sence
        has_repetable_blocks = column_types.get("has_repetable_blocks", False)
        has_carto_fields = column_types.get("has_carto_fields", False)
        
        log(f"Types de colonnes d√©tect√©s:")
        log(f"  - Colonnes dossiers: {len(column_types['dossier'])}")
        log(f"  - Colonnes champs: {len(column_types['champs'])}")
        log(f"  - Colonnes annotations: {len(column_types['annotations'])}")
        log(f"  - Blocs r√©p√©tables d√©tect√©s: {'Oui' if has_repetable_blocks else 'Non'}")
        log(f"  - Champs cartographiques d√©tect√©s: {'Oui' if has_carto_fields else 'Non'}")
        
        if has_repetable_blocks and "repetable_rows" in column_types:
            log_verbose(f"  - Colonnes blocs r√©p√©tables: {len(column_types['repetable_rows'])}")

        # Cr√©er ou r√©cup√©rer les tables Grist pour la d√©marche
        table_ids = client.create_or_clear_grist_tables(demarche_number, column_types)
        
        # Log des table IDs
        log(f"Tables utilis√©es pour l'importation:")
        log(f"  Table dossiers: {table_ids['dossier_table_id']}")
        log(f"  Table champs: {table_ids['champ_table_id']}")
        log(f"  Table annotations: {table_ids['annotation_table_id']}")
        if table_ids.get('repetable_table_id'):
            log(f"  Table blocs r√©p√©tables: {table_ids['repetable_table_id']}")
        else:
            log_verbose(f"  Table blocs r√©p√©tables: Non cr√©√©e (aucun bloc r√©p√©table d√©tect√©)")

        # Nouvelle logique de traitement par lots
        batch_size = 100  # Ajustez selon les performances
        success_count = 0
        error_count = 0
        
        # Organiser les dossiers en lots
        dossier_batches = []
        for i in range(0, total_dossiers, batch_size):
            batch = dossiers[i:min(i+batch_size, total_dossiers)]
            dossier_batches.append(batch)
        
        log(f"Dossiers organis√©s en {len(dossier_batches)} lots")
        
        # Traiter chaque lot
        for batch_idx, batch in enumerate(dossier_batches):
            log(f"Traitement du lot {batch_idx+1}/{len(dossier_batches)} ({len(batch)} dossiers)...")
            
            # Pr√©parer les donn√©es pour les tables
            dossier_records = []
            champ_records = []
            annotation_records = []
            
            batch_success = 0
            batch_errors = 0
            dossier_batch_data = []  # Pour stocker les donn√©es compl√®tes des dossiers pour les blocs r√©p√©tables
            
            # R√©cup√©rer les donn√©es d√©taill√©es pour chaque dossier du lot
            for dossier_brief in batch:
                dossier_number = dossier_brief["number"]
                try:
                    # R√©cup√©rer les donn√©es compl√®tes du dossier
                    dossier_data = get_dossier(dossier_number)
                    
                    # V√©rifier si le dictionnaire est vide (dossier inaccessible)
                    if not dossier_data:
                        log_error(f"Dossier {dossier_number} inaccessible en raison de restrictions de permission, ignor√©")
                        batch_errors += 1
                        continue
                    
                    # Garder les donn√©es compl√®tes pour le traitement des blocs r√©p√©tables
                    if has_repetable_blocks:
                        dossier_batch_data.append(dossier_data)
                    
                    # Extraire les donn√©es pour les 3 tables principales
                    exclude_repetition = has_repetable_blocks  # N'exclure que si on va les traiter s√©par√©ment
                    flat_data = dossier_to_flat_data(dossier_data, exclude_repetition_champs=exclude_repetition, problematic_ids=problematic_descriptor_ids)
                    
                    # Pr√©parer les donn√©es pour la table des dossiers
                    dossier_info = flat_data["dossier"]
                    dossier_record = {}
                    
                    for column in column_types["dossier"]:
                        field_id = column["id"]
                        field_type = column["type"]
                        
                        if field_id in dossier_info:
                            value = dossier_info[field_id]
                        elif "dossier_" + field_id in dossier_info:
                            value = dossier_info["dossier_" + field_id]
                        else:
                            continue
                        
                        dossier_record[field_id] = format_value_for_grist(value, field_type)

                    # Ajouter explicitement l'ID du dossier
                    if "dossier_id" in dossier_info:
                        dossier_record["dossier_id"] = dossier_info["dossier_id"]
                    
                    # V√©rifier que "number" est pr√©sent
                    if "number" not in dossier_record:
                        dossier_record["number"] = dossier_number
                    
                    # Traitement des labels
                    if "labels" not in dossier_data or not dossier_data.get("labels"):
                        # Si les labels ne sont pas pr√©sents, essayer de les r√©cup√©rer s√©par√©ment
                        labels = get_dossier_labels(dossier_number)
                        
                        if labels:
                            # Cr√©er label_names
                            label_names = [label.get("name", "") for label in labels if label.get("name")]
                            dossier_record["label_names"] = ", ".join(label_names) if label_names else ""
                            
                            # Cr√©er labels_json
                            labels_with_colors = [
                                {
                                    "id": label.get("id", ""),
                                    "name": label.get("name", ""),
                                    "color": label.get("color", "")
                                }
                                for label in labels if label.get("name") and label.get("color")
                            ]
                            
                            if labels_with_colors:
                                import json
                                dossier_record["labels_json"] = json_module.dumps(labels_with_colors, ensure_ascii=False)
                            else:
                                dossier_record["labels_json"] = ""
                    
                    # Ajouter √† la liste des dossiers √† traiter
                    dossier_records.append(dossier_record)
                    
                    # Pr√©parer les donn√©es pour la table des champs
                    champ_record = {"dossier_number": dossier_number}
                    champ_column_types = {col["id"]: col["type"] for col in column_types["champs"]}
                    
                    for champ in flat_data["champs"]:
                        # Ignorer les champs probl√©matiques
                        if champ["type"] in ["HeaderSectionChamp", "ExplicationChamp"]:
                            continue
                        
                        champ_label = normalize_column_name(champ["label"])
                        value = champ.get("value", "")
                        
                        # Pour les types complexes, utiliser la repr√©sentation JSON
                        if champ["type"] in ["CarteChamp", "AddressChamp", "SiretChamp"] and champ.get("json_value"):
                            try:
                                value = json_module.dumps(champ["json_value"], ensure_ascii=False)
                            except (TypeError, ValueError):
                                value = str(champ["json_value"])
                        
                        column_type = champ_column_types.get(champ_label, "Text")
                        champ_record[champ_label] = format_value_for_grist(value, column_type)
                    
                    champ_records.append(champ_record)
                    
                    # Pr√©parer les donn√©es pour la table des annotations
                    annotation_record = {"dossier_number": dossier_number}
                    annotation_column_types = {col["id"]: col["type"] for col in column_types["annotations"]}
                    
                    for annotation in flat_data["annotations"]:
                        # Ignorer les annotations probl√©matiques
                        if annotation["type"] in ["HeaderSectionChamp", "ExplicationChamp"]:
                            continue
                        
                        # Pour la table des annotations, enlever le pr√©fixe "annotation_"
                        original_label = annotation["label"]
                        if original_label.startswith("annotation_"):
                            annotation_label = normalize_column_name(original_label[11:])
                        else:
                            annotation_label = normalize_column_name(original_label)
                        
                        value = annotation.get("value", "")
                        
                        # Pour les types complexes, utiliser la repr√©sentation JSON
                        if annotation["type"] in ["CarteChamp", "AddressChamp", "SiretChamp"] and annotation.get("json_value"):
                            try:
                                value = json_module.dumps(annotation["json_value"], ensure_ascii=False)
                            except (TypeError, ValueError):
                                value = str(annotation["json_value"])
                        
                        column_type = annotation_column_types.get(annotation_label, "Text")
                        annotation_record[annotation_label] = format_value_for_grist(value, column_type)
                    
                    annotation_records.append(annotation_record)
                    
                    batch_success += 1
                except Exception as e:
                    log_error(f"Exception lors du traitement du dossier {dossier_number}: {str(e)}")
                    import traceback
                    traceback.print_exc()
                    batch_errors += 1
            
            # Effectuer les op√©rations d'upsert par lot
            
            # Variables pour suivre les succ√®s de ce lot
            batch_successful_dossiers = set()
            batch_failed_dossiers = set()
            
            # 1. Table des dossiers
            if dossier_records:
                log(f"  Upsert par lot de {len(dossier_records)} dossiers...")
                success = client.upsert_multiple_dossiers_in_grist(table_ids["dossier_table_id"], dossier_records)
                
                # Mettre √† jour les ensembles de dossiers selon le r√©sultat
                for record in dossier_records:
                    dossier_num = record.get("number") or record.get("dossier_number")
                    if dossier_num:
                        if success:
                            batch_successful_dossiers.add(str(dossier_num))
                        else:
                            batch_failed_dossiers.add(str(dossier_num))
            
            # 2. Table des champs
            if champ_records:
                log(f"  Upsert par lot de {len(champ_records)} enregistrements de champs...")
                success_champs = client.upsert_multiple_dossiers_in_grist(table_ids["champ_table_id"], champ_records)
                
                # Pour les champs, on ne compte les √©checs que s'il y a vraiment eu un probl√®me
                # car les champs sont trait√©s individuellement et peuvent r√©ussir partiellement
                if not success_champs:
                    log_verbose("Probl√®me partiel lors du traitement des champs, mais les dossiers principaux restent valides")
            
            # 3. Table des annotations
            if annotation_records:
                log(f"  Upsert par lot de {len(annotation_records)} enregistrements d'annotations...")
                success_annotations = client.upsert_multiple_dossiers_in_grist(table_ids["annotation_table_id"], annotation_records)
                
                # Pour les annotations, m√™me logique que pour les champs
                if not success_annotations:
                    log_verbose("Probl√®me partiel lors du traitement des annotations, mais les dossiers principaux restent valides")
            
            # 4. Traiter les blocs r√©p√©tables si n√©cessaire
            if has_repetable_blocks and table_ids.get("repetable_table_id") and dossier_batch_data:
                repetable_table_id = table_ids["repetable_table_id"]
                repetable_column_types = column_types.get("repetable_rows", [])
                
                log(f"  Traitement des blocs r√©p√©tables pour {len(dossier_batch_data)} dossiers...")
                
                try:
                    import repetable_processor as rp
                    success_count_rep, error_count_rep = rp.process_repetables_batch(
                        client,
                        list(dossier_batch_data.values()),
                        repetable_table_id,
                        repetable_column_types,
                        problematic_ids=problematic_descriptor_ids,
                        batch_size=batch_size
                    )
                    log(f"  Blocs r√©p√©tables trait√©s par lot: {success_count_rep} r√©ussis, {error_count_rep} en √©chec")
                except Exception as e:
                    log_error(f"  Erreur lors du traitement des blocs r√©p√©tables: {str(e)}")
                    import traceback
                    traceback.print_exc()
            
            # Mettre √† jour les ensembles globaux avec les r√©sultats de ce lot
            successful_dossiers.update(batch_successful_dossiers)
            failed_dossiers.update(batch_failed_dossiers)
            
            # Log de progression avec les vrais chiffres
            current_success = len(successful_dossiers)
            current_failed = len(failed_dossiers)
            
            log(f"  Lot {batch_idx+1} termin√©: {len(batch_successful_dossiers)} dossiers trait√©s avec succ√®s, {len(batch_failed_dossiers)} en √©chec")
            log(f"  Progression: {current_success}/{total_dossiers} dossiers trait√©s ({current_success/total_dossiers*100:.1f}%)")
        
        # Afficher le r√©sum√© final
        log("\nTraitement termin√©!")
        log(f"Dossiers trait√©s avec succ√®s: {success_count}/{total_dossiers}")
        if error_count > 0:
            log(f"Dossiers en √©chec: {error_count}")
        
        return True
        
    except Exception as e:
        log_error(f"Erreur lors du traitement de la d√©marche pour Grist: {e}")
        import traceback
        traceback.print_exc()
        return False

# Fonction optimis√©e pour le traitement d'une d√©marche pour Grist (Possibilit√© d'augmenter ou de diminuer batch_size et max_workers)
# Cette fonction est con√ßue pour √™tre plus rapide et plus efficace, en utilisant le traitement par lots et le traitement parall√®le.
def process_demarche_for_grist_optimized(client, demarche_number, parallel=True, batch_size=100, max_workers=3, api_filters=None):
    """
    Version optimis√©e du traitement d'une d√©marche pour Grist avec filtrage c√¥t√© serveur.
    
    Args:
        client: Instance de GristClient
        demarche_number: Num√©ro de la d√©marche
        parallel: Utiliser le traitement parall√®le si True
        batch_size: Taille des lots pour le traitement par lot
        max_workers: Nombre maximum de workers pour le traitement parall√®le
        api_filters: Filtres optimis√©s √† appliquer c√¥t√© serveur (NOUVEAU)
        
    Returns:
        bool: Succ√®s ou √©chec global
    """
    try:
        start_time = time.time()
        
        # Initialiser des ensembles pour suivre les dossiers trait√©s
        successful_dossiers = set()
        failed_dossiers = set()
        
        # Initialiser le cache de colonnes
        column_cache = ColumnCache(client)
        
        # V√©rifier que le document Grist existe
        try:
            doc_info = client.get_document_info()
            doc_name = doc_info.get("name", client.doc_id) if isinstance(doc_info, dict) else "Document ID " + client.doc_id
            log(f"Document Grist trouv√©: {doc_name}")
        except Exception as e:
            log_error(f"Erreur lors de la v√©rification du document Grist: {e}")
            return False
        
        # M√©thode avanc√©e: R√©cup√©rer le sch√©ma complet de la d√©marche
        problematic_descriptor_ids = set()
        column_types = None
        schema_method_successful = False
        
        # Essayer d'abord la m√©thode bas√©e sur le sch√©ma
        log(f"R√©cup√©ration du sch√©ma complet de la d√©marche {demarche_number}...")
        try:
            if 'get_demarche_schema' in globals() and 'create_columns_from_schema' in globals():
                demarche_schema = get_optimized_schema(demarche_number)
                log_schema_improvements(demarche_schema, demarche_number)
                log(f"Sch√©ma r√©cup√©r√© avec succ√®s pour la d√©marche: {demarche_schema['title']}")
                
                # G√©n√©rer les d√©finitions de colonnes √† partir du sch√©ma complet
                column_types, problematic_descriptor_ids = create_columns_from_schema(demarche_schema)
                
                # R√©cup√©rer les indicateurs de pr√©sence
                has_repetable_blocks = column_types.get("has_repetable_blocks", False)
                has_carto_fields = column_types.get("has_carto_fields", False)
                
                log(f"Identificateurs de {len(problematic_descriptor_ids)} descripteurs probl√©matiques √† filtrer")
                log(f"Types de colonnes d√©tect√©s √† partir du sch√©ma:")
                log(f"  - Colonnes dossiers: {len(column_types['dossier'])}")
                log(f"  - Colonnes champs: {len(column_types['champs'])}")
                log(f"  - Colonnes annotations: {len(column_types['annotations'])}")
                log(f"  - Blocs r√©p√©tables d√©tect√©s: {'Oui' if has_repetable_blocks else 'Non'}")
                log(f"  - Champs cartographiques d√©tect√©s: {'Oui' if has_carto_fields else 'Non'}")
                
                if has_repetable_blocks and "repetable_rows" in column_types:
                    log_verbose(f"  - Colonnes blocs r√©p√©tables: {len(column_types['repetable_rows'])}")
                
                # Marquer la m√©thode comme r√©ussie
                schema_method_successful = True
            else:
                log("M√©thode bas√©e sur le sch√©ma non disponible, utilisation de la m√©thode alternative...")
        except Exception as e:
            log_error(f"Erreur lors de la r√©cup√©ration du sch√©ma: {str(e)}")
            import traceback
            traceback.print_exc()
            log("Utilisation de la m√©thode alternative avec √©chantillons de dossiers...")
        
        # Essayer d'utiliser la m√©thode de mise √† jour qui pr√©serve les donn√©es existantes
        try:
            if 'update_grist_tables_from_schema' in globals():
                log("Mise √† jour des tables Grist en pr√©servant les donn√©es existantes...")
                table_result = update_grist_tables_from_schema(client, demarche_number, column_types if schema_method_successful else None, problematic_descriptor_ids)
                
                # Convertir le format de retour pour compatibilit√©
                table_ids = {
                    "dossier_table_id": table_result.get("dossiers"),
                    "champ_table_id": table_result.get("champs"), 
                    "annotation_table_id": table_result.get("annotations"),
                    "repetable_table_id": table_result.get("repetable_rows")
                }
            else:
                # M√©thode classique qui peut effacer des donn√©es
                log("Utilisation de la m√©thode classique de cr√©ation/modification de tables")
                table_ids = client.create_or_clear_grist_tables(demarche_number, column_types if schema_method_successful else None)
        except Exception as e:
            log_error(f"Erreur lors de la mise √† jour des tables Grist: {str(e)}")
            # Fallback sur la m√©thode classique si pas de column_types
            if not schema_method_successful:
                # R√©cup√©rer les IDs des champs probl√©matiques √† filtrer
                problematic_descriptor_ids = get_problematic_descriptor_ids(demarche_number)
                log(f"Filtrage de {len(problematic_descriptor_ids)} descripteurs probl√©matiques")
                
                # R√©cup√©rer quelques dossiers pour analyse du sch√©ma
                sample_dossiers = []
                sample_dossier_numbers = []
                
                # Utiliser l'ancienne m√©thode pour r√©cup√©rer des √©chantillons
                try:
                    from queries_graphql import get_demarche_dossiers
                    all_dossiers_brief = get_demarche_dossiers(demarche_number)
                    sample_size = min(3, len(all_dossiers_brief))
                    sample_dossier_numbers = [all_dossiers_brief[i]["number"] for i in range(sample_size)]
                    
                    for num in sample_dossier_numbers:
                        dossier = get_dossier(num)
                        if dossier:
                            sample_dossiers.append(dossier)
                except Exception as e:
                    log_error(f"Erreur lors de la r√©cup√©ration des √©chantillons: {e}")
                    return False
                
                if not sample_dossiers:
                    log_error("Aucun dossier n'a pu √™tre r√©cup√©r√© pour l'analyse du sch√©ma")
                    return False
                
                # D√©tecter les types de colonnes
                log("D√©tection des types de colonnes...")
                column_types = detect_column_types_from_multiple_dossiers(sample_dossiers, problematic_ids=problematic_descriptor_ids)
            
            # Fallback sur la m√©thode classique
            log("Fallback sur la m√©thode classique de cr√©ation/modification de tables")
            table_ids = client.create_or_clear_grist_tables(demarche_number, column_types)
        
        # Log des table IDs
        log(f"Tables utilis√©es pour l'importation:")
        log(f"  Table dossiers: {table_ids['dossier_table_id']}")
        log(f"  Table champs: {table_ids['champ_table_id']}")
        log(f"  Table annotations: {table_ids['annotation_table_id']}")
        if table_ids.get('repetable_table_id'):
            log(f"  Table blocs r√©p√©tables: {table_ids['repetable_table_id']}")
        
        # ========================================
        # NOUVELLE SECTION : R√âCUP√âRATION OPTIMIS√âE DES DOSSIERS
        # ========================================
        
        if api_filters and api_filters:
            # Utiliser la r√©cup√©ration optimis√©e avec filtres c√¥t√© serveur
            log(f"[FILTRAGE] R√©cup√©ration optimis√©e des dossiers avec filtres c√¥t√© serveur...")
        # V√©rifier les filtres pass√©s
        if api_filters.get('groupes_instructeurs'):
            log(f"Filtre par groupes instructeurs (num√©ros): {', '.join(map(str, api_filters['groupes_instructeurs']))}")
        if api_filters.get('statuts'):
            log(f"Filtre par statuts: {', '.join(api_filters['statuts'])}")
        if api_filters.get('date_debut'):
            log(f"Filtre par date de d√©but: {api_filters['date_debut']}")
        if api_filters.get('date_fin'):
            log(f"Filtre par date de fin: {api_filters['date_fin']}")
            
            all_dossiers = get_demarche_dossiers_filtered(
                demarche_number,
                date_debut=api_filters.get('date_debut'),
                date_fin=api_filters.get('date_fin'),
                groupes_instructeurs=api_filters.get('groupes_instructeurs'),
                statuts=api_filters.get('statuts')
            )
            
            total_dossiers = len(all_dossiers)
            log(f"[OK] Dossiers r√©cup√©r√©s avec filtres optimis√©s: {total_dossiers}")
            
            # Pas besoin de filtrage c√¥t√© client car d√©j√† fait c√¥t√© serveur
            filtered_dossiers = all_dossiers
            
        else:
            # Utiliser l'ancienne m√©thode avec filtrage c√¥t√© client
            log(f"[ATTENTION] R√©cup√©ration classique de tous les dossiers (pas de filtres optimis√©s)")
            
            # R√©cup√©rer les filtres depuis les variables d'environnement pour compatibilit√©
            date_debut_str = os.getenv("DATE_DEPOT_DEBUT", "")
            date_fin_str = os.getenv("DATE_DEPOT_FIN", "")
            statuts_filter = os.getenv("STATUTS_DOSSIERS", "").split(",") if os.getenv("STATUTS_DOSSIERS") else []
            groupes_filter = os.getenv("GROUPES_INSTRUCTEURS", "").split(",") if os.getenv("GROUPES_INSTRUCTEURS") else []
            
            # Nettoyer les filtres
            if date_debut_str.strip() == "":
                date_debut_str = None
            if date_fin_str.strip() == "":
                date_fin_str = None
            statuts_filter = [s for s in statuts_filter if s.strip()]
            groupes_filter = [g for g in groupes_filter if g.strip()]
            
            # Convertir les dates
            date_debut = None
            date_fin = None
            if date_debut_str:
                try:
                    date_debut = datetime.strptime(date_debut_str, "%Y-%m-%d")
                    log(f"Filtre par date de d√©but: {date_debut.strftime('%Y-%m-%d')}")
                except ValueError:
                    log_error(f"Format de date de d√©but invalide: {date_debut_str}")
            
            if date_fin_str:
                try:
                    date_fin = datetime.strptime(date_fin_str, "%Y-%m-%d")  
                    log(f"Filtre par date de fin: {date_fin.strftime('%Y-%m-%d')}")
                except ValueError:
                    log_error(f"Format de date de fin invalide: {date_fin_str}")
            
            if statuts_filter:
                log(f"Filtre par statuts: {', '.join(statuts_filter)}")
            if groupes_filter:
                log(f"Filtre par groupes instructeurs: {', '.join(groupes_filter)}")
            
            # R√©cup√©rer tous les dossiers puis filtrer c√¥t√© client
            from queries_graphql import get_demarche_dossiers
            log(f"R√©cup√©ration de tous les dossiers avec pagination...")
            all_dossiers = get_demarche_dossiers(demarche_number)
            
            total_dossiers_brut = len(all_dossiers)
            log(f"Nombre total de dossiers trouv√©s: {total_dossiers_brut}")
            
            # Appliquer les filtres c√¥t√© client
            filtered_dossiers = []
            for dossier in all_dossiers:
                # Filtre par statut
                if statuts_filter and dossier["state"] not in statuts_filter:
                    continue
                    
                # Filtre par groupe instructeur
                if groupes_filter and (
                    not dossier.get("groupeInstructeur") or 
                    str(dossier["groupeInstructeur"].get("number", "")) not in groupes_filter
                ):
                    continue
                    
                # Filtre par date de d√©p√¥t
                if date_debut or date_fin:
                    date_depot_str = dossier.get("dateDepot")
                    if not date_depot_str:
                        continue
                    
                    try:
                        date_depot = datetime.strptime(date_depot_str.split("T")[0], "%Y-%m-%d")
                        
                        if date_debut and date_depot < date_debut:
                            continue
                        if date_fin and date_depot > date_fin:
                            continue
                    except (ValueError, AttributeError, TypeError):
                        continue
                
                filtered_dossiers.append(dossier)
            
            total_dossiers = len(filtered_dossiers)
            log(f"Apr√®s filtrage: {total_dossiers} dossiers ({(total_dossiers/total_dossiers_brut*100) if total_dossiers_brut > 0 else 0:.1f}%)")
        
        # Si aucun dossier ne correspond aux crit√®res, c'est quand m√™me un succ√®s (les tables sont cr√©√©es)
        if total_dossiers == 0:
            log("Aucun dossier ne correspond aux crit√®res de filtrage")
            elapsed_time = time.time() - start_time
            minutes = int(elapsed_time // 60)
            seconds = elapsed_time % 60
            log("\nTraitement termin√©!")
            log(f"Dur√©e totale: {minutes} min {seconds:.1f} sec")
            log("Tables cr√©√©es avec succ√®s, mais aucun dossier √† traiter.")
            return True

        # Organiser les dossiers en lots
        dossier_batches = []
        batch_count = (total_dossiers + batch_size - 1) // batch_size
        
        for i in range(0, total_dossiers, batch_size):
            batch_dossier_numbers = [filtered_dossiers[j]["number"] for j in range(i, min(i+batch_size, total_dossiers))]
            dossier_batches.append(batch_dossier_numbers)
        
        log(f"Dossiers organis√©s en {batch_count} lots de {batch_size} maximum")
        
        # Le reste du traitement reste identique...
        # [COPIER LE RESTE DE LA FONCTION EXISTANTE DEPUIS "# Traiter les lots de dossiers"]
        
        # Traiter les lots de dossiers
        total_success = 0
        total_errors = 0
        
        for batch_idx, batch in enumerate(dossier_batches):
            log(f"Traitement du lot {batch_idx+1}/{batch_count} ({len(batch)} dossiers)...")
            
            # R√©cup√©rer les dossiers complets
            if parallel:
                batch_dossiers_dict = fetch_dossiers_in_parallel(batch, max_workers=max_workers)
            else:
                batch_dossiers_dict = {}
                for num in batch:
                    dossier = get_dossier(num)
                    if dossier:
                        batch_dossiers_dict[num] = dossier
                    else:
                        log_error(f"Dossier {num} inaccessible en raison de restrictions de permission, ignor√©")
            
            if not batch_dossiers_dict:
                log_error(f"Aucun dossier n'a pu √™tre r√©cup√©r√© pour le lot {batch_idx+1}")
                continue
            
            # Pr√©parer les dossiers pour upsert par lot
            dossier_records = []
            champ_records = []
            annotation_records = []
            
            for dossier_num, dossier_data in batch_dossiers_dict.items():
                try:
                    # Extraire les donn√©es √† plat
                    exclude_repetition = column_types.get("has_repetable_blocks", False)
                    flat_data = dossier_to_flat_data(dossier_data, exclude_repetition_champs=exclude_repetition, problematic_ids=problematic_descriptor_ids)
                    
                    # Pr√©parer l'enregistrement pour la table des dossiers
                    dossier_info = flat_data["dossier"]
                    dossier_record = {}
                    for column in column_types["dossier"]:
                        field_id = column["id"]
                        field_type = column["type"]
                        
                        if field_id in dossier_info:
                            value = dossier_info[field_id]
                        elif "dossier_" + field_id in dossier_info:
                            value = dossier_info["dossier_" + field_id]
                        else:
                            continue
                        
                        dossier_record[field_id] = format_value_for_grist(value, field_type)
                    
                    # V√©rifier que number est pr√©sent
                    if "number" not in dossier_record:
                        dossier_record["number"] = dossier_num
                        
                    # Traitement des labels
                    if "labels" not in dossier_data or not dossier_data.get("labels"):
                        labels = get_dossier_labels(dossier_num)
                        
                        if labels:
                            # Cr√©er label_names
                            label_names = [label.get("name", "") for label in labels if label.get("name")]
                            dossier_record["label_names"] = ", ".join(label_names) if label_names else ""
                            
                            # Cr√©er labels_json
                            labels_with_colors = [
                                {
                                    "id": label.get("id", ""),
                                    "name": label.get("name", ""),
                                    "color": label.get("color", "")
                                }
                                for label in labels if label.get("name") and label.get("color")
                            ]
                            
                            if labels_with_colors:
                                dossier_record["labels_json"] = json_module.dumps(labels_with_colors, ensure_ascii=False)
                            else:
                                dossier_record["labels_json"] = ""
                    
                    dossier_records.append(dossier_record)
                    
                    # Pr√©parer l'enregistrement pour la table des champs
                    champ_record = {"dossier_number": dossier_num}
                    champ_column_types = {col["id"]: col["type"] for col in column_types["champs"]}
                    
                    # Collecter les IDs des champs pour les stocker
                    champ_ids = []
                    for champ in flat_data["champs"]:
                        if champ.get("id"):
                            champ_ids.append(str(champ["id"]))
                    if champ_ids:
                        champ_record["champ_id"] = "_".join(champ_ids)
                    
                    for champ in flat_data["champs"]:
                        if champ["type"] in ["HeaderSectionChamp", "ExplicationChamp"]:
                            continue
                            
                        normalized_label = normalize_column_name(champ["label"])
                        
                        value = champ.get("value", "")
                        if champ["type"] in ["CarteChamp", "AddressChamp", "SiretChamp"] and champ.get("json_value"):
                            try:
                                value = json_module.dumps(champ["json_value"], ensure_ascii=False)
                            except:
                                value = str(champ["json_value"])
                        
                        column_type = champ_column_types.get(normalized_label, "Text")
                        champ_record[normalized_label] = format_value_for_grist(value, column_type)
                    
                    champ_records.append(champ_record)
                    
                    # Pr√©parer l'enregistrement pour la table des annotations
                    annotation_record = {"dossier_number": dossier_num}
                    annotation_column_types = {col["id"]: col["type"] for col in column_types["annotations"]}
                    
                    # Cr√©er dynamiquement les colonnes pour les IDs des annotations
                    add_id_columns_based_on_annotations(client, table_ids["annotation_table_id"], flat_data["annotations"])
                    
                    # Collecter les IDs des annotations pour les stocker
                    annotation_ids = []
                    for annotation in flat_data["annotations"]:
                        if annotation.get("id"):
                            annotation_ids.append(str(annotation["id"]))
                    if annotation_ids:
                        annotation_record["annotation_id"] = "_".join(annotation_ids)
                    
                    for annotation in flat_data["annotations"]:
                        if annotation["type"] in ["HeaderSectionChamp", "ExplicationChamp"]:
                            continue
                            
                        original_label = annotation["label"]
                        if original_label.startswith("annotation_"):
                            normalized_label = normalize_column_name(original_label[11:])
                        else:
                            normalized_label = normalize_column_name(original_label)
                        
                        value = annotation.get("value", "")
                        if annotation["type"] in ["CarteChamp", "AddressChamp", "SiretChamp"] and annotation.get("json_value"):
                            try:
                                value = json_module.dumps(annotation["json_value"], ensure_ascii=False)
                            except:
                                value = str(annotation["json_value"])
                        
                        column_type = annotation_column_types.get(normalized_label, "Text")
                        annotation_record[normalized_label] = format_value_for_grist(value, column_type)
                        
                        # Ajouter l'ID de l'annotation
                        if "id" in annotation:
                            id_column = f"{normalized_label}_id"
                            annotation_record[id_column] = annotation["id"]
                    
                    annotation_records.append(annotation_record)
                    
                except Exception as e:
                    log_error(f"Erreur lors de la pr√©paration du dossier {dossier_num}: {str(e)}")
                    import traceback
                    traceback.print_exc()
            
            # Effectuer les op√©rations d'upsert par lot
            if dossier_records:
                log(f"  Upsert par lot de {len(dossier_records)} dossiers...")
                success = client.upsert_multiple_dossiers_in_grist(table_ids["dossier_table_id"], dossier_records)

                # Mettre √† jour les ensembles de dossiers
                for record in dossier_records:
                    dossier_num = record.get("number") or record.get("dossier_number")
                    if dossier_num:
                        if success:
                            successful_dossiers.add(str(dossier_num))
                        else:
                            failed_dossiers.add(str(dossier_num))
            
            if champ_records:
                log(f"  Upsert par lot de {len(champ_records)} enregistrements de champs...")
                success = client.upsert_multiple_dossiers_in_grist(table_ids["champ_table_id"], champ_records)
                if success:
                    total_success += len(champ_records)
                else:
                    total_errors += len(champ_records)
            
            if annotation_records:
                log(f"  Upsert par lot de {len(annotation_records)} enregistrements d'annotations...")
                success = client.upsert_multiple_dossiers_in_grist(table_ids["annotation_table_id"], annotation_records)
            
            # Traiter les blocs r√©p√©tables si n√©cessaire
            if column_types.get("has_repetable_blocks", False) and table_ids.get("repetable_table_id") and "repetable_rows" in column_types:
                repetable_table_id = table_ids["repetable_table_id"]
                repetable_column_types = column_types.get("repetable_rows", [])
                
                try:
                    import repetable_processor as rp
                    success_count, error_count = rp.process_repetables_batch(
                        client,
                        list(batch_dossiers_dict.values()),
                        repetable_table_id,
                        repetable_column_types,
                        problematic_ids=problematic_descriptor_ids,
                        batch_size=batch_size
                    )
                    log(f"  Blocs r√©p√©tables trait√©s par lot: {success_count} r√©ussis, {error_count} en √©chec")
                except Exception as e:
                    log_error(f"  Erreur lors du traitement des blocs r√©p√©tables: {str(e)}")
                    import traceback
                    traceback.print_exc()
        
        # Calculer les statistiques finales
        elapsed_time = time.time() - start_time
        minutes = int(elapsed_time // 60)
        seconds = elapsed_time % 60

        # Calculer les nombres √† partir des ensembles
        total_success = len(successful_dossiers)
        total_errors = len(failed_dossiers)

        log("\nTraitement termin√©!")
        log(f"Dur√©e totale: {minutes} min {seconds:.1f} sec")
        log(f"Dossiers trait√©s avec succ√®s: {total_success}")
        if total_errors > 0:
            log(f"Dossiers en √©chec: {total_errors}")
                
        return total_success > 0 or schema_method_successful
        
    except Exception as e:
        log_error(f"Erreur lors du traitement de la d√©marche pour Grist: {e}")
        import traceback
        traceback.print_exc()
        return False
    
def main():
    load_dotenv()

    grist_base_url = os.getenv("GRIST_BASE_URL")
    grist_api_key = os.getenv("GRIST_API_KEY")
    grist_doc_id = os.getenv("GRIST_DOC_ID")

    if not all([grist_base_url, grist_api_key, grist_doc_id]):
        log_error("Configuration Grist incompl√®te dans le fichier .env")
        log("Assurez-vous d'avoir d√©fini GRIST_BASE_URL, GRIST_API_KEY et GRIST_DOC_ID")
        return 1

    # Masquer partiellement la cl√© API par s√©curit√©
    api_key_masked = grist_api_key[:4] + "..." + grist_api_key[-4:] if len(grist_api_key) > 8 else "***"
    log(f"Configuration Grist:")
    log(f"  URL de base: {grist_base_url}")
    log(f"  Cl√© API: {api_key_masked}")
    log(f"  ID du document: {grist_doc_id}")

    # R√©cup√©rer le num√©ro de d√©marche
    demarche_number = os.getenv("DEMARCHE_NUMBER")
    if not demarche_number:
        log_error("DEMARCHE_NUMBER non d√©fini dans le fichier .env")
        return 1

    try:
        # Convertir le num√©ro de d√©marche en entier
        demarche_number = int(demarche_number)
        log(f"Traitement de la d√©marche: {demarche_number}")
    except ValueError:
        log_error("DEMARCHE_NUMBER doit √™tre un nombre entier")
        return 1

    # Initialiser le client Grist
    client = GristClient(grist_base_url, grist_api_key, grist_doc_id)

    # NOUVEAU : R√©cup√©rer les filtres optimis√©s depuis l'environnement
    api_filters_json = os.getenv('API_FILTERS_JSON', '{}')
    try:
        api_filters = json_module.loads(api_filters_json)
        if api_filters:
            log(f"[FILTRAGE] Filtres optimis√©s d√©tect√©s: {list(api_filters.keys())}")
    except:
        api_filters = {}
        log("‚ö†Ô∏è Aucun filtre optimis√© d√©tect√©, utilisation de l'ancienne m√©thode")

    # R√©cup√©rer les autres param√®tres
    parallel = os.getenv('PARALLEL', 'true').lower() == 'true'
    batch_size = int(os.getenv('BATCH_SIZE', '50'))
    max_workers = int(os.getenv('MAX_WORKERS', '3'))

    # Traiter la d√©marche avec la fonction optimis√©e
    if process_demarche_for_grist_optimized(
        client, 
        demarche_number, 
        parallel=parallel, 
        batch_size=batch_size, 
        max_workers=max_workers,
        api_filters=api_filters  # Passer les filtres optimis√©s
    ):
        log(f"Traitement de la d√©marche {demarche_number} termin√© avec succ√®s")
        return 0
    else:
        log_error(f"√âchec du traitement de la d√©marche {demarche_number}")
        return 1

if __name__ == "__main__":
    sys.exit(main())
